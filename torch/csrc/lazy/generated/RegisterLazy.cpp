// required for old g++ to compile PRId64 macros, see
// https://github.com/pytorch/pytorch/issues/3571
// for context
#ifndef __STDC_FORMAT_MACROS
#define __STDC_FORMAT_MACROS
#endif

// an external backend might generate file within its code tree
// and check all the source files within the tree with clang-format.
// so, disable it since the backend might have a different config.
// clang-format off

// NOTE: This condition is true for all PyTorch internal libraries, it
//       just excludes external projects such as torch_xla which
//       re-use some of the PyTorch codegen machinery.
#if defined(CAFFE2_BUILD_MAIN_LIB)        || \
    defined(TORCH_CUDA_BUILD_MAIN_LIB)    || \
    defined(TORCH_HIP_BUILD_MAIN_LIB)     || \
    defined(TORCH_CUDA_CU_BUILD_MAIN_LIB) || \
    defined(TORCH_CUDA_CPP_BUILD_MAIN_LIB)
#define TORCH_ASSERT_ONLY_METHOD_OPERATORS
#endif

// @generated by tools/codegen/gen.py from RegisterDispatchKey.cpp

#include <c10/core/TensorImpl.h>
#include <c10/core/Allocator.h>
#include <ATen/DeviceGuard.h>
#include <ATen/NamedTensorUtils.h>
#include <ATen/Utils.h>
#include <ATen/WrapDimUtils.h>
#include <ATen/Dispatch.h>
#include <c10/util/ExclusivelyOwned.h>
#include <c10/util/Half.h>
#include <c10/core/UndefinedTensorImpl.h>
#include <c10/util/Optional.h>
#include <ATen/Tensor.h>
#include <ATen/native/Resize.h>

#include <cstddef>
#include <functional>
#include <memory>
#include <utility>

#include <ATen/Config.h>
#include <ATen/core/op_registration/adaption.h>
#include <torch/library.h>

#include <torch/csrc/lazy/generated/LazyNativeFunctions.h>
#include <ATen/ops/as_strided_native.h>
#include <ATen/ops/empty.h>
#include <ATen/ops/empty_strided.h>
#include <ATen/ops/_copy_from_and_resize.h>
#include <ATen/ops/_copy_from.h>



namespace at {

// NB: TORCH_LIBRARY_IMPL must be in an anonymous namespace to avoid
// ambiguity with conflicting identifiers that may have been defined in
// at namespace already.
namespace {


void resize_out(const Tensor &out, IntArrayRef sizes, IntArrayRef strides, const TensorOptions &options) {
  TORCH_CHECK(options.dtype() == out.dtype(),
      "Expected out tensor to have dtype ", options.dtype(), ", but got ", out.dtype(), " instead");
  TORCH_CHECK(options.device() == out.device(),
      "Expected out tensor to have device ", options.device(), ", but got ", out.device(), " instead");
  const bool resized = at::native::resize_output(out, sizes);
  // Only restride if a resize occurred; otherwise we ignore the (advisory)
  // strides from the meta function and directly use the output tensor's
  // preexisting strides
  if (resized) {
    if (!strides.empty()) {
      TORCH_INTERNAL_ASSERT(!options.memory_format_opt().has_value());
      at::native::as_strided_(out, sizes, strides);
    } else if (options.memory_format_opt().has_value()) {
      out.unsafeGetTensorImpl()->empty_tensor_restride(*options.memory_format_opt());
    }
  }
}

void check_inplace(const Tensor &self, IntArrayRef sizes, const TensorOptions &options) {
  // These checks are needed on those operators that:
  //   1) don't use 'TensorIterator' (e.g. 'addmm' and 'baddbmm')
  //   2) have particular typing rules (e.g. 'cumsum' and 'cumprod')
  // For other operators (e.g. 'add'), 'TensorIterator' already checks
  // these things separately.
  TORCH_CHECK(options.dtype() == self.dtype(),
      "Bad in-place call: ",
      "input tensor dtype ", self.dtype(), " and output tensor dtype ", options.dtype(), " should match");
  TORCH_CHECK(options.device() == self.device(),
      "Bad in-place call: ",
      "input tensor device ", self.device(), " and output tensor device ", options.device(), " should match");
  TORCH_CHECK(sizes == self.sizes(),
      "Bad in-place call: ",
      "input tensor size ", self.sizes(), " and output tensor size ", sizes, " should match");
}

namespace {

at::Tensor wrapper___adaptive_avg_pool2d(const at::Tensor & self, at::IntArrayRef output_size) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::_adaptive_avg_pool2d(self, output_size);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___adaptive_avg_pool2d_backward(const at::Tensor & grad_output, const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::_adaptive_avg_pool2d_backward(grad_output, self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___copy_from(const at::Tensor & self, const at::Tensor & dst, bool non_blocking) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::_copy_from(self, dst, non_blocking);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___copy_from_and_resize(const at::Tensor & self, const at::Tensor & dst) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::_copy_from_and_resize(self, dst);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___log_softmax(const at::Tensor & self, int64_t dim, bool half_to_float) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::_log_softmax(self, dim, half_to_float);
}

} // anonymous namespace
at::Tensor & wrapper_out__log_softmax_out(const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out) {
  auto wrapper_out__log_softmax_out_tmp = wrapper___log_softmax(self, dim, half_to_float);
  at::_copy_from_and_resize(wrapper_out__log_softmax_out_tmp, out);
  return out;
}
namespace {

at::Tensor wrapper___log_softmax_backward_data(const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, at::ScalarType input_dtype) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::_log_softmax_backward_data(grad_output, output, dim, input_dtype);
}

} // anonymous namespace
at::Tensor & wrapper_out__log_softmax_backward_data_out(const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, at::ScalarType input_dtype, at::Tensor & out) {
  auto wrapper_out__log_softmax_backward_data_out_tmp = wrapper___log_softmax_backward_data(grad_output, output, dim, input_dtype);
  at::_copy_from_and_resize(wrapper_out__log_softmax_backward_data_out_tmp, out);
  return out;
}
namespace {

at::Tensor wrapper___softmax(const at::Tensor & self, int64_t dim, bool half_to_float) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::_softmax(self, dim, half_to_float);
}

} // anonymous namespace
at::Tensor & wrapper_out__softmax_out(const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out) {
  auto wrapper_out__softmax_out_tmp = wrapper___softmax(self, dim, half_to_float);
  at::_copy_from_and_resize(wrapper_out__softmax_out_tmp, out);
  return out;
}
namespace {

at::Tensor wrapper___softmax_backward_data(const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, at::ScalarType input_dtype) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::_softmax_backward_data(grad_output, output, dim, input_dtype);
}

} // anonymous namespace
at::Tensor & wrapper_out__softmax_backward_data_out(const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, at::ScalarType input_dtype, at::Tensor & grad_input) {
  auto wrapper_out__softmax_backward_data_out_tmp = wrapper___softmax_backward_data(grad_output, output, dim, input_dtype);
  at::_copy_from_and_resize(wrapper_out__softmax_backward_data_out_tmp, grad_input);
  return grad_input;
}
namespace {

at::Tensor wrapper___to_copy(const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, bool non_blocking, c10::optional<at::MemoryFormat> memory_format) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::_to_copy(self, dtype, layout, device, pin_memory, non_blocking, memory_format);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___unsafe_view(const at::Tensor & self, at::IntArrayRef size) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::_unsafe_view(self, size);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__abs(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::abs(self);
}

} // anonymous namespace
at::Tensor & wrapper_out_abs_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_out_abs_out_tmp = wrapper__abs(self);
  at::_copy_from_and_resize(wrapper_out_abs_out_tmp, out);
  return out;
}
at::Tensor & wrapper__abs_(at::Tensor & self) {
  auto wrapper__abs__tmp = wrapper__abs(self);
  at::_copy_from(wrapper__abs__tmp, self);
  return self;
}
namespace {

at::Tensor wrapper_Tensor_add(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::add(self, other, alpha);
}

} // anonymous namespace
at::Tensor & wrapper_out_add_out(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha, at::Tensor & out) {
  auto wrapper_out_add_out_tmp = wrapper_Tensor_add(self, other, alpha);
  at::_copy_from_and_resize(wrapper_out_add_out_tmp, out);
  return out;
}
at::Tensor & wrapper_Tensor_add_(at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
  auto wrapper_Tensor_add__tmp = wrapper_Tensor_add(self, other, alpha);
  at::_copy_from(wrapper_Tensor_add__tmp, self);
  return self;
}
namespace {

at::Tensor wrapper__addcdiv(const at::Tensor & self, const at::Tensor & tensor1, const at::Tensor & tensor2, const at::Scalar & value) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::addcdiv(self, tensor1, tensor2, value);
}

} // anonymous namespace
at::Tensor & wrapper_out_addcdiv_out(const at::Tensor & self, const at::Tensor & tensor1, const at::Tensor & tensor2, const at::Scalar & value, at::Tensor & out) {
  auto wrapper_out_addcdiv_out_tmp = wrapper__addcdiv(self, tensor1, tensor2, value);
  at::_copy_from_and_resize(wrapper_out_addcdiv_out_tmp, out);
  return out;
}
at::Tensor & wrapper__addcdiv_(at::Tensor & self, const at::Tensor & tensor1, const at::Tensor & tensor2, const at::Scalar & value) {
  auto wrapper__addcdiv__tmp = wrapper__addcdiv(self, tensor1, tensor2, value);
  at::_copy_from(wrapper__addcdiv__tmp, self);
  return self;
}
namespace {

at::Tensor wrapper__addcmul(const at::Tensor & self, const at::Tensor & tensor1, const at::Tensor & tensor2, const at::Scalar & value) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::addcmul(self, tensor1, tensor2, value);
}

} // anonymous namespace
at::Tensor & wrapper_out_addcmul_out(const at::Tensor & self, const at::Tensor & tensor1, const at::Tensor & tensor2, const at::Scalar & value, at::Tensor & out) {
  auto wrapper_out_addcmul_out_tmp = wrapper__addcmul(self, tensor1, tensor2, value);
  at::_copy_from_and_resize(wrapper_out_addcmul_out_tmp, out);
  return out;
}
at::Tensor & wrapper__addcmul_(at::Tensor & self, const at::Tensor & tensor1, const at::Tensor & tensor2, const at::Scalar & value) {
  auto wrapper__addcmul__tmp = wrapper__addcmul(self, tensor1, tensor2, value);
  at::_copy_from(wrapper__addcmul__tmp, self);
  return self;
}
namespace {

at::Tensor wrapper__addmm(const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::addmm(self, mat1, mat2, beta, alpha);
}

} // anonymous namespace
at::Tensor & wrapper_out_addmm_out(const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
  auto wrapper_out_addmm_out_tmp = wrapper__addmm(self, mat1, mat2, beta, alpha);
  at::_copy_from_and_resize(wrapper_out_addmm_out_tmp, out);
  return out;
}
at::Tensor & wrapper__addmm_(at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha) {
  auto wrapper__addmm__tmp = wrapper__addmm(self, mat1, mat2, beta, alpha);
  at::_copy_from(wrapper__addmm__tmp, self);
  return self;
}
namespace {

at::Tensor wrapper__alias(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::alias(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__all(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::all(self);
}

} // anonymous namespace
at::Tensor & wrapper_all_out_all_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_all_out_all_out_tmp = wrapper__all(self);
  at::_copy_from_and_resize(wrapper_all_out_all_out_tmp, out);
  return out;
}
namespace {

at::Tensor wrapper__any(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::any(self);
}

} // anonymous namespace
at::Tensor & wrapper_all_out_any_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_all_out_any_out_tmp = wrapper__any(self);
  at::_copy_from_and_resize(wrapper_all_out_any_out_tmp, out);
  return out;
}
namespace {

at::Tensor & wrapper_start_out_arange_out(const at::Scalar & start, const at::Scalar & end, const at::Scalar & step, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::arange_out(start, end, step, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__as_strided(const at::Tensor & self, at::IntArrayRef size, at::IntArrayRef stride, c10::optional<int64_t> storage_offset) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::as_strided(self, size, stride, storage_offset);
}

} // anonymous namespace
namespace {

const at::Tensor & wrapper__as_strided_(const at::Tensor & self, at::IntArrayRef size, at::IntArrayRef stride, c10::optional<int64_t> storage_offset) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::as_strided_(self, size, stride, storage_offset);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__avg_pool2d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::avg_pool2d(self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
}

} // anonymous namespace
at::Tensor & wrapper_out_avg_pool2d_out(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override, at::Tensor & out) {
  auto wrapper_out_avg_pool2d_out_tmp = wrapper__avg_pool2d(self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
  at::_copy_from_and_resize(wrapper_out_avg_pool2d_out_tmp, out);
  return out;
}
namespace {

at::Tensor wrapper__avg_pool2d_backward(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::avg_pool2d_backward(grad_output, self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
}

} // anonymous namespace
at::Tensor & wrapper_grad_input_avg_pool2d_backward_out(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override, at::Tensor & grad_input) {
  auto wrapper_grad_input_avg_pool2d_backward_out_tmp = wrapper__avg_pool2d_backward(grad_output, self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
  at::_copy_from_and_resize(wrapper_grad_input_avg_pool2d_backward_out_tmp, grad_input);
  return grad_input;
}
namespace {

at::Tensor wrapper__baddbmm(const at::Tensor & self, const at::Tensor & batch1, const at::Tensor & batch2, const at::Scalar & beta, const at::Scalar & alpha) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::baddbmm(self, batch1, batch2, beta, alpha);
}

} // anonymous namespace
at::Tensor & wrapper_out_baddbmm_out(const at::Tensor & self, const at::Tensor & batch1, const at::Tensor & batch2, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
  auto wrapper_out_baddbmm_out_tmp = wrapper__baddbmm(self, batch1, batch2, beta, alpha);
  at::_copy_from_and_resize(wrapper_out_baddbmm_out_tmp, out);
  return out;
}
at::Tensor & wrapper__baddbmm_(at::Tensor & self, const at::Tensor & batch1, const at::Tensor & batch2, const at::Scalar & beta, const at::Scalar & alpha) {
  auto wrapper__baddbmm__tmp = wrapper__baddbmm(self, batch1, batch2, beta, alpha);
  at::_copy_from(wrapper__baddbmm__tmp, self);
  return self;
}
namespace {

at::Tensor wrapper__bernoulli(const at::Tensor & self, c10::optional<at::Generator> generator) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::bernoulli(self, generator);
}

} // anonymous namespace
at::Tensor & wrapper_out_bernoulli_out(const at::Tensor & self, c10::optional<at::Generator> generator, at::Tensor & out) {
  auto wrapper_out_bernoulli_out_tmp = wrapper__bernoulli(self, generator);
  at::_copy_from_and_resize(wrapper_out_bernoulli_out_tmp, out);
  return out;
}
namespace {

at::Tensor & wrapper_float_bernoulli_(at::Tensor & self, double p, c10::optional<at::Generator> generator) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::bernoulli_(self, p, generator);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__binary_cross_entropy(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::binary_cross_entropy(self, target, weight, reduction);
}

} // anonymous namespace
at::Tensor & wrapper_out_binary_cross_entropy_out(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, at::Tensor & out) {
  auto wrapper_out_binary_cross_entropy_out_tmp = wrapper__binary_cross_entropy(self, target, weight, reduction);
  at::_copy_from_and_resize(wrapper_out_binary_cross_entropy_out_tmp, out);
  return out;
}
namespace {

at::Tensor wrapper__binary_cross_entropy_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::binary_cross_entropy_backward(grad_output, self, target, weight, reduction);
}

} // anonymous namespace
at::Tensor & wrapper_grad_input_binary_cross_entropy_backward_out(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, at::Tensor & grad_input) {
  auto wrapper_grad_input_binary_cross_entropy_backward_out_tmp = wrapper__binary_cross_entropy_backward(grad_output, self, target, weight, reduction);
  at::_copy_from_and_resize(wrapper_grad_input_binary_cross_entropy_backward_out_tmp, grad_input);
  return grad_input;
}
namespace {

at::Tensor wrapper_Tensor_bitwise_and(const at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::bitwise_and(self, other);
}

} // anonymous namespace
at::Tensor & wrapper_Tensor_out_bitwise_and_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto wrapper_Tensor_out_bitwise_and_out_tmp = wrapper_Tensor_bitwise_and(self, other);
  at::_copy_from_and_resize(wrapper_Tensor_out_bitwise_and_out_tmp, out);
  return out;
}
at::Tensor & wrapper_Tensor_bitwise_and_(at::Tensor & self, const at::Tensor & other) {
  auto wrapper_Tensor_bitwise_and__tmp = wrapper_Tensor_bitwise_and(self, other);
  at::_copy_from(wrapper_Tensor_bitwise_and__tmp, self);
  return self;
}
namespace {

at::Tensor wrapper_Tensor_bitwise_or(const at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::bitwise_or(self, other);
}

} // anonymous namespace
at::Tensor & wrapper_Tensor_out_bitwise_or_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto wrapper_Tensor_out_bitwise_or_out_tmp = wrapper_Tensor_bitwise_or(self, other);
  at::_copy_from_and_resize(wrapper_Tensor_out_bitwise_or_out_tmp, out);
  return out;
}
at::Tensor & wrapper_Tensor_bitwise_or_(at::Tensor & self, const at::Tensor & other) {
  auto wrapper_Tensor_bitwise_or__tmp = wrapper_Tensor_bitwise_or(self, other);
  at::_copy_from(wrapper_Tensor_bitwise_or__tmp, self);
  return self;
}
namespace {

at::Tensor wrapper__bmm(const at::Tensor & self, const at::Tensor & mat2) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::bmm(self, mat2);
}

} // anonymous namespace
at::Tensor & wrapper_out_bmm_out(const at::Tensor & self, const at::Tensor & mat2, at::Tensor & out) {
  auto wrapper_out_bmm_out_tmp = wrapper__bmm(self, mat2);
  at::_copy_from_and_resize(wrapper_out_bmm_out_tmp, out);
  return out;
}
namespace {

at::Tensor wrapper__cat(at::TensorList tensors, int64_t dim) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::cat(tensors, dim);
}

} // anonymous namespace
at::Tensor & wrapper_out_cat_out(at::TensorList tensors, int64_t dim, at::Tensor & out) {
  auto wrapper_out_cat_out_tmp = wrapper__cat(tensors, dim);
  at::_copy_from_and_resize(wrapper_out_cat_out_tmp, out);
  return out;
}
namespace {

at::Tensor wrapper__clamp(const at::Tensor & self, const c10::optional<at::Scalar> & min, const c10::optional<at::Scalar> & max) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::clamp(self, min, max);
}

} // anonymous namespace
at::Tensor & wrapper_out_clamp_out(const at::Tensor & self, const c10::optional<at::Scalar> & min, const c10::optional<at::Scalar> & max, at::Tensor & out) {
  auto wrapper_out_clamp_out_tmp = wrapper__clamp(self, min, max);
  at::_copy_from_and_resize(wrapper_out_clamp_out_tmp, out);
  return out;
}
at::Tensor & wrapper__clamp_(at::Tensor & self, const c10::optional<at::Scalar> & min, const c10::optional<at::Scalar> & max) {
  auto wrapper__clamp__tmp = wrapper__clamp(self, min, max);
  at::_copy_from(wrapper__clamp__tmp, self);
  return self;
}
namespace {

at::Tensor wrapper__clamp_min(const at::Tensor & self, const at::Scalar & min) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::clamp_min(self, min);
}

} // anonymous namespace
at::Tensor & wrapper_out_clamp_min_out(const at::Tensor & self, const at::Scalar & min, at::Tensor & out) {
  auto wrapper_out_clamp_min_out_tmp = wrapper__clamp_min(self, min);
  at::_copy_from_and_resize(wrapper_out_clamp_min_out_tmp, out);
  return out;
}
at::Tensor & wrapper__clamp_min_(at::Tensor & self, const at::Scalar & min) {
  auto wrapper__clamp_min__tmp = wrapper__clamp_min(self, min);
  at::_copy_from(wrapper__clamp_min__tmp, self);
  return self;
}
namespace {

at::Tensor wrapper__clone(const at::Tensor & self, c10::optional<at::MemoryFormat> memory_format) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::clone(self, memory_format);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__constant_pad_nd(const at::Tensor & self, at::IntArrayRef pad, const at::Scalar & value) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::constant_pad_nd(self, pad, value);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__convolution(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool transposed, at::IntArrayRef output_padding, int64_t groups) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::convolution(input, weight, bias, stride, padding, dilation, transposed, output_padding, groups);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor,at::Tensor> wrapper__convolution_backward(const at::Tensor & grad_output, const at::Tensor & input, const at::Tensor & weight, c10::optional<at::IntArrayRef> bias_sizes, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool transposed, at::IntArrayRef output_padding, int64_t groups, ::std::array<bool,3> output_mask) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::convolution_backward(grad_output, input, weight, bias_sizes, stride, padding, dilation, transposed, output_padding, groups, output_mask);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__cos(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::cos(self);
}

} // anonymous namespace
at::Tensor & wrapper_out_cos_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_out_cos_out_tmp = wrapper__cos(self);
  at::_copy_from_and_resize(wrapper_out_cos_out_tmp, out);
  return out;
}
at::Tensor & wrapper__cos_(at::Tensor & self) {
  auto wrapper__cos__tmp = wrapper__cos(self);
  at::_copy_from(wrapper__cos__tmp, self);
  return self;
}
namespace {

at::Tensor wrapper__cumsum(const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::cumsum(self, dim, dtype);
}

} // anonymous namespace
at::Tensor & wrapper_out_cumsum_out(const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
  auto wrapper_out_cumsum_out_tmp = wrapper__cumsum(self, dim, dtype);
  at::_copy_from_and_resize(wrapper_out_cumsum_out_tmp, out);
  return out;
}
at::Tensor & wrapper__cumsum_(at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
  auto wrapper__cumsum__tmp = wrapper__cumsum(self, dim, dtype);
  at::_copy_from(wrapper__cumsum__tmp, self);
  return self;
}
namespace {

at::Tensor wrapper_Tensor_div(const at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::div(self, other);
}

} // anonymous namespace
at::Tensor & wrapper_out_div_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto wrapper_out_div_out_tmp = wrapper_Tensor_div(self, other);
  at::_copy_from_and_resize(wrapper_out_div_out_tmp, out);
  return out;
}
at::Tensor & wrapper_Tensor_div_(at::Tensor & self, const at::Tensor & other) {
  auto wrapper_Tensor_div__tmp = wrapper_Tensor_div(self, other);
  at::_copy_from(wrapper_Tensor_div__tmp, self);
  return self;
}
namespace {

at::Tensor wrapper_Tensor_mode_div(const at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::div(self, other, rounding_mode);
}

} // anonymous namespace
at::Tensor & wrapper_out_mode_div_out(const at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode, at::Tensor & out) {
  auto wrapper_out_mode_div_out_tmp = wrapper_Tensor_mode_div(self, other, rounding_mode);
  at::_copy_from_and_resize(wrapper_out_mode_div_out_tmp, out);
  return out;
}
at::Tensor & wrapper_Tensor_mode_div_(at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode) {
  auto wrapper_Tensor_mode_div__tmp = wrapper_Tensor_mode_div(self, other, rounding_mode);
  at::_copy_from(wrapper_Tensor_mode_div__tmp, self);
  return self;
}
namespace {

at::Tensor wrapper__elu(const at::Tensor & self, const at::Scalar & alpha, const at::Scalar & scale, const at::Scalar & input_scale) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::elu(self, alpha, scale, input_scale);
}

} // anonymous namespace
at::Tensor & wrapper_out_elu_out(const at::Tensor & self, const at::Scalar & alpha, const at::Scalar & scale, const at::Scalar & input_scale, at::Tensor & out) {
  auto wrapper_out_elu_out_tmp = wrapper__elu(self, alpha, scale, input_scale);
  at::_copy_from_and_resize(wrapper_out_elu_out_tmp, out);
  return out;
}
at::Tensor & wrapper__elu_(at::Tensor & self, const at::Scalar & alpha, const at::Scalar & scale, const at::Scalar & input_scale) {
  auto wrapper__elu__tmp = wrapper__elu(self, alpha, scale, input_scale);
  at::_copy_from(wrapper__elu__tmp, self);
  return self;
}
namespace {

at::Tensor wrapper__elu_backward(const at::Tensor & grad_output, const at::Scalar & alpha, const at::Scalar & scale, const at::Scalar & input_scale, bool is_result, const at::Tensor & self_or_result) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::elu_backward(grad_output, alpha, scale, input_scale, is_result, self_or_result);
}

} // anonymous namespace
at::Tensor & wrapper_grad_input_elu_backward_out(const at::Tensor & grad_output, const at::Scalar & alpha, const at::Scalar & scale, const at::Scalar & input_scale, bool is_result, const at::Tensor & self_or_result, at::Tensor & grad_input) {
  auto wrapper_grad_input_elu_backward_out_tmp = wrapper__elu_backward(grad_output, alpha, scale, input_scale, is_result, self_or_result);
  at::_copy_from_and_resize(wrapper_grad_input_elu_backward_out_tmp, grad_input);
  return grad_input;
}
namespace {

at::Tensor wrapper__embedding(const at::Tensor & weight, const at::Tensor & indices, int64_t padding_idx, bool scale_grad_by_freq, bool sparse) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::embedding(weight, indices, padding_idx, scale_grad_by_freq, sparse);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__embedding_dense_backward(const at::Tensor & grad_output, const at::Tensor & indices, int64_t num_weights, int64_t padding_idx, bool scale_grad_by_freq) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::embedding_dense_backward(grad_output, indices, num_weights, padding_idx, scale_grad_by_freq);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_memory_format_empty(at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::empty(size, dtype, layout, device, pin_memory, memory_format);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__empty_strided(at::IntArrayRef size, at::IntArrayRef stride, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::empty_strided(size, stride, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Scalar_eq(const at::Tensor & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::eq(self, other);
}

} // anonymous namespace
at::Tensor & wrapper_Scalar_out_eq_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  auto wrapper_Scalar_out_eq_out_tmp = wrapper_Scalar_eq(self, other);
  at::_copy_from_and_resize(wrapper_Scalar_out_eq_out_tmp, out);
  return out;
}
at::Tensor & wrapper_Scalar_eq_(at::Tensor & self, const at::Scalar & other) {
  auto wrapper_Scalar_eq__tmp = wrapper_Scalar_eq(self, other);
  at::_copy_from(wrapper_Scalar_eq__tmp, self);
  return self;
}
namespace {

at::Tensor wrapper_Tensor_eq(const at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::eq(self, other);
}

} // anonymous namespace
at::Tensor & wrapper_Tensor_out_eq_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto wrapper_Tensor_out_eq_out_tmp = wrapper_Tensor_eq(self, other);
  at::_copy_from_and_resize(wrapper_Tensor_out_eq_out_tmp, out);
  return out;
}
at::Tensor & wrapper_Tensor_eq_(at::Tensor & self, const at::Tensor & other) {
  auto wrapper_Tensor_eq__tmp = wrapper_Tensor_eq(self, other);
  at::_copy_from(wrapper_Tensor_eq__tmp, self);
  return self;
}
namespace {

at::Tensor wrapper__exp(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::exp(self);
}

} // anonymous namespace
at::Tensor & wrapper_out_exp_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_out_exp_out_tmp = wrapper__exp(self);
  at::_copy_from_and_resize(wrapper_out_exp_out_tmp, out);
  return out;
}
at::Tensor & wrapper__exp_(at::Tensor & self) {
  auto wrapper__exp__tmp = wrapper__exp(self);
  at::_copy_from(wrapper__exp__tmp, self);
  return self;
}
namespace {

at::Tensor wrapper__expand(const at::Tensor & self, at::IntArrayRef size, bool implicit) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::expand(self, size, implicit);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Scalar_fill_(at::Tensor & self, const at::Scalar & value) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::fill_(self, value);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__flip(const at::Tensor & self, at::IntArrayRef dims) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::flip(self, dims);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__floor(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::floor(self);
}

} // anonymous namespace
at::Tensor & wrapper_out_floor_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_out_floor_out_tmp = wrapper__floor(self);
  at::_copy_from_and_resize(wrapper_out_floor_out_tmp, out);
  return out;
}
at::Tensor & wrapper__floor_(at::Tensor & self) {
  auto wrapper__floor__tmp = wrapper__floor(self);
  at::_copy_from(wrapper__floor__tmp, self);
  return self;
}
namespace {

at::Tensor wrapper__frac(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::frac(self);
}

} // anonymous namespace
at::Tensor & wrapper_out_frac_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_out_frac_out_tmp = wrapper__frac(self);
  at::_copy_from_and_resize(wrapper_out_frac_out_tmp, out);
  return out;
}
at::Tensor & wrapper__frac_(at::Tensor & self) {
  auto wrapper__frac__tmp = wrapper__frac(self);
  at::_copy_from(wrapper__frac__tmp, self);
  return self;
}
namespace {

at::Tensor wrapper__gather(const at::Tensor & self, int64_t dim, const at::Tensor & index, bool sparse_grad) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::gather(self, dim, index, sparse_grad);
}

} // anonymous namespace
at::Tensor & wrapper_out_gather_out(const at::Tensor & self, int64_t dim, const at::Tensor & index, bool sparse_grad, at::Tensor & out) {
  auto wrapper_out_gather_out_tmp = wrapper__gather(self, dim, index, sparse_grad);
  at::_copy_from_and_resize(wrapper_out_gather_out_tmp, out);
  return out;
}
namespace {

at::Tensor wrapper_Scalar_ge(const at::Tensor & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::ge(self, other);
}

} // anonymous namespace
at::Tensor & wrapper_Scalar_out_ge_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  auto wrapper_Scalar_out_ge_out_tmp = wrapper_Scalar_ge(self, other);
  at::_copy_from_and_resize(wrapper_Scalar_out_ge_out_tmp, out);
  return out;
}
at::Tensor & wrapper_Scalar_ge_(at::Tensor & self, const at::Scalar & other) {
  auto wrapper_Scalar_ge__tmp = wrapper_Scalar_ge(self, other);
  at::_copy_from(wrapper_Scalar_ge__tmp, self);
  return self;
}
namespace {

at::Tensor wrapper_Tensor_ge(const at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::ge(self, other);
}

} // anonymous namespace
at::Tensor & wrapper_Tensor_out_ge_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto wrapper_Tensor_out_ge_out_tmp = wrapper_Tensor_ge(self, other);
  at::_copy_from_and_resize(wrapper_Tensor_out_ge_out_tmp, out);
  return out;
}
at::Tensor & wrapper_Tensor_ge_(at::Tensor & self, const at::Tensor & other) {
  auto wrapper_Tensor_ge__tmp = wrapper_Tensor_ge(self, other);
  at::_copy_from(wrapper_Tensor_ge__tmp, self);
  return self;
}
namespace {

at::Tensor wrapper__gelu(const at::Tensor & self, c10::string_view approximate) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::gelu(self, approximate);
}

} // anonymous namespace
at::Tensor & wrapper_out_gelu_out(const at::Tensor & self, c10::string_view approximate, at::Tensor & out) {
  auto wrapper_out_gelu_out_tmp = wrapper__gelu(self, approximate);
  at::_copy_from_and_resize(wrapper_out_gelu_out_tmp, out);
  return out;
}
namespace {

at::Tensor wrapper__gelu_backward(const at::Tensor & grad_output, const at::Tensor & self, c10::string_view approximate) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::gelu_backward(grad_output, self, approximate);
}

} // anonymous namespace
at::Tensor & wrapper_grad_input_gelu_backward_out(const at::Tensor & grad_output, const at::Tensor & self, c10::string_view approximate, at::Tensor & grad_input) {
  auto wrapper_grad_input_gelu_backward_out_tmp = wrapper__gelu_backward(grad_output, self, approximate);
  at::_copy_from_and_resize(wrapper_grad_input_gelu_backward_out_tmp, grad_input);
  return grad_input;
}
namespace {

at::Tensor wrapper__glu(const at::Tensor & self, int64_t dim) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::glu(self, dim);
}

} // anonymous namespace
at::Tensor & wrapper_out_glu_out(const at::Tensor & self, int64_t dim, at::Tensor & out) {
  auto wrapper_out_glu_out_tmp = wrapper__glu(self, dim);
  at::_copy_from_and_resize(wrapper_out_glu_out_tmp, out);
  return out;
}
namespace {

at::Tensor wrapper__glu_backward(const at::Tensor & grad_output, const at::Tensor & self, int64_t dim) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::glu_backward(grad_output, self, dim);
}

} // anonymous namespace
at::Tensor & wrapper_grad_input_glu_backward_out(const at::Tensor & grad_output, const at::Tensor & self, int64_t dim, at::Tensor & grad_input) {
  auto wrapper_grad_input_glu_backward_out_tmp = wrapper__glu_backward(grad_output, self, dim);
  at::_copy_from_and_resize(wrapper_grad_input_glu_backward_out_tmp, grad_input);
  return grad_input;
}
namespace {

at::Tensor wrapper__grid_sampler_2d(const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::grid_sampler_2d(input, grid, interpolation_mode, padding_mode, align_corners);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper__grid_sampler_2d_backward(const at::Tensor & grad_output, const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners, ::std::array<bool,2> output_mask) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::grid_sampler_2d_backward(grad_output, input, grid, interpolation_mode, padding_mode, align_corners, output_mask);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Scalar_gt(const at::Tensor & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::gt(self, other);
}

} // anonymous namespace
at::Tensor & wrapper_Scalar_out_gt_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  auto wrapper_Scalar_out_gt_out_tmp = wrapper_Scalar_gt(self, other);
  at::_copy_from_and_resize(wrapper_Scalar_out_gt_out_tmp, out);
  return out;
}
at::Tensor & wrapper_Scalar_gt_(at::Tensor & self, const at::Scalar & other) {
  auto wrapper_Scalar_gt__tmp = wrapper_Scalar_gt(self, other);
  at::_copy_from(wrapper_Scalar_gt__tmp, self);
  return self;
}
namespace {

at::Tensor wrapper_Tensor_gt(const at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::gt(self, other);
}

} // anonymous namespace
at::Tensor & wrapper_Tensor_out_gt_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto wrapper_Tensor_out_gt_out_tmp = wrapper_Tensor_gt(self, other);
  at::_copy_from_and_resize(wrapper_Tensor_out_gt_out_tmp, out);
  return out;
}
at::Tensor & wrapper_Tensor_gt_(at::Tensor & self, const at::Tensor & other) {
  auto wrapper_Tensor_gt__tmp = wrapper_Tensor_gt(self, other);
  at::_copy_from(wrapper_Tensor_gt__tmp, self);
  return self;
}
namespace {

at::Tensor wrapper__hardsigmoid(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::hardsigmoid(self);
}

} // anonymous namespace
at::Tensor & wrapper_out_hardsigmoid_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_out_hardsigmoid_out_tmp = wrapper__hardsigmoid(self);
  at::_copy_from_and_resize(wrapper_out_hardsigmoid_out_tmp, out);
  return out;
}
at::Tensor & wrapper__hardsigmoid_(at::Tensor & self) {
  auto wrapper__hardsigmoid__tmp = wrapper__hardsigmoid(self);
  at::_copy_from(wrapper__hardsigmoid__tmp, self);
  return self;
}
namespace {

at::Tensor wrapper__index_select(const at::Tensor & self, int64_t dim, const at::Tensor & index) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::index_select(self, dim, index);
}

} // anonymous namespace
at::Tensor & wrapper_out_index_select_out(const at::Tensor & self, int64_t dim, const at::Tensor & index, at::Tensor & out) {
  auto wrapper_out_index_select_out_tmp = wrapper__index_select(self, dim, index);
  at::_copy_from_and_resize(wrapper_out_index_select_out_tmp, out);
  return out;
}
namespace {

at::Tensor wrapper__kl_div_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, bool log_target) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::kl_div_backward(grad_output, self, target, reduction, log_target);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__l1_loss_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::l1_loss_backward(grad_output, self, target, reduction);
}

} // anonymous namespace
at::Tensor & wrapper_grad_input_l1_loss_backward_out(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & grad_input) {
  auto wrapper_grad_input_l1_loss_backward_out_tmp = wrapper__l1_loss_backward(grad_output, self, target, reduction);
  at::_copy_from_and_resize(wrapper_grad_input_l1_loss_backward_out_tmp, grad_input);
  return grad_input;
}
namespace {

at::Tensor wrapper_Scalar_le(const at::Tensor & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::le(self, other);
}

} // anonymous namespace
at::Tensor & wrapper_Scalar_out_le_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  auto wrapper_Scalar_out_le_out_tmp = wrapper_Scalar_le(self, other);
  at::_copy_from_and_resize(wrapper_Scalar_out_le_out_tmp, out);
  return out;
}
at::Tensor & wrapper_Scalar_le_(at::Tensor & self, const at::Scalar & other) {
  auto wrapper_Scalar_le__tmp = wrapper_Scalar_le(self, other);
  at::_copy_from(wrapper_Scalar_le__tmp, self);
  return self;
}
namespace {

at::Tensor wrapper_Tensor_le(const at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::le(self, other);
}

} // anonymous namespace
at::Tensor & wrapper_Tensor_out_le_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto wrapper_Tensor_out_le_out_tmp = wrapper_Tensor_le(self, other);
  at::_copy_from_and_resize(wrapper_Tensor_out_le_out_tmp, out);
  return out;
}
at::Tensor & wrapper_Tensor_le_(at::Tensor & self, const at::Tensor & other) {
  auto wrapper_Tensor_le__tmp = wrapper_Tensor_le(self, other);
  at::_copy_from(wrapper_Tensor_le__tmp, self);
  return self;
}
namespace {

at::Tensor wrapper__leaky_relu(const at::Tensor & self, const at::Scalar & negative_slope) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::leaky_relu(self, negative_slope);
}

} // anonymous namespace
at::Tensor & wrapper_out_leaky_relu_out(const at::Tensor & self, const at::Scalar & negative_slope, at::Tensor & out) {
  auto wrapper_out_leaky_relu_out_tmp = wrapper__leaky_relu(self, negative_slope);
  at::_copy_from_and_resize(wrapper_out_leaky_relu_out_tmp, out);
  return out;
}
at::Tensor & wrapper__leaky_relu_(at::Tensor & self, const at::Scalar & negative_slope) {
  auto wrapper__leaky_relu__tmp = wrapper__leaky_relu(self, negative_slope);
  at::_copy_from(wrapper__leaky_relu__tmp, self);
  return self;
}
namespace {

at::Tensor wrapper__leaky_relu_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & negative_slope, bool self_is_result) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::leaky_relu_backward(grad_output, self, negative_slope, self_is_result);
}

} // anonymous namespace
at::Tensor & wrapper_grad_input_leaky_relu_backward_out(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & negative_slope, bool self_is_result, at::Tensor & grad_input) {
  auto wrapper_grad_input_leaky_relu_backward_out_tmp = wrapper__leaky_relu_backward(grad_output, self, negative_slope, self_is_result);
  at::_copy_from_and_resize(wrapper_grad_input_leaky_relu_backward_out_tmp, grad_input);
  return grad_input;
}
namespace {

at::Tensor wrapper__log(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::log(self);
}

} // anonymous namespace
at::Tensor & wrapper_out_log_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_out_log_out_tmp = wrapper__log(self);
  at::_copy_from_and_resize(wrapper_out_log_out_tmp, out);
  return out;
}
at::Tensor & wrapper__log_(at::Tensor & self) {
  auto wrapper__log__tmp = wrapper__log(self);
  at::_copy_from(wrapper__log__tmp, self);
  return self;
}
namespace {

at::Tensor wrapper__log2(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::log2(self);
}

} // anonymous namespace
at::Tensor & wrapper_out_log2_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_out_log2_out_tmp = wrapper__log2(self);
  at::_copy_from_and_resize(wrapper_out_log2_out_tmp, out);
  return out;
}
at::Tensor & wrapper__log2_(at::Tensor & self) {
  auto wrapper__log2__tmp = wrapper__log2(self);
  at::_copy_from(wrapper__log2__tmp, self);
  return self;
}
namespace {

at::Tensor wrapper__log_sigmoid_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & buffer) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::log_sigmoid_backward(grad_output, self, buffer);
}

} // anonymous namespace
at::Tensor & wrapper_grad_input_log_sigmoid_backward_out(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & buffer, at::Tensor & grad_input) {
  auto wrapper_grad_input_log_sigmoid_backward_out_tmp = wrapper__log_sigmoid_backward(grad_output, self, buffer);
  at::_copy_from_and_resize(wrapper_grad_input_log_sigmoid_backward_out_tmp, grad_input);
  return grad_input;
}
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper__log_sigmoid_forward(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::log_sigmoid_forward(self);
}

} // anonymous namespace
::std::tuple<at::Tensor &,at::Tensor &> wrapper_output_log_sigmoid_forward_out(const at::Tensor & self, at::Tensor & output, at::Tensor & buffer) {
  auto wrapper_output_log_sigmoid_forward_out_tmp = wrapper__log_sigmoid_forward(self);
  at::_copy_from_and_resize(std::get<0>(wrapper_output_log_sigmoid_forward_out_tmp), output);
  at::_copy_from_and_resize(std::get<1>(wrapper_output_log_sigmoid_forward_out_tmp), buffer);
  return ::std::tuple<at::Tensor &,at::Tensor &>(output, buffer);
}
namespace {

at::Tensor wrapper__logdet(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::logdet(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Scalar_lt(const at::Tensor & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::lt(self, other);
}

} // anonymous namespace
at::Tensor & wrapper_Scalar_out_lt_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  auto wrapper_Scalar_out_lt_out_tmp = wrapper_Scalar_lt(self, other);
  at::_copy_from_and_resize(wrapper_Scalar_out_lt_out_tmp, out);
  return out;
}
at::Tensor & wrapper_Scalar_lt_(at::Tensor & self, const at::Scalar & other) {
  auto wrapper_Scalar_lt__tmp = wrapper_Scalar_lt(self, other);
  at::_copy_from(wrapper_Scalar_lt__tmp, self);
  return self;
}
namespace {

at::Tensor wrapper_Tensor_lt(const at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::lt(self, other);
}

} // anonymous namespace
at::Tensor & wrapper_Tensor_out_lt_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto wrapper_Tensor_out_lt_out_tmp = wrapper_Tensor_lt(self, other);
  at::_copy_from_and_resize(wrapper_Tensor_out_lt_out_tmp, out);
  return out;
}
at::Tensor & wrapper_Tensor_lt_(at::Tensor & self, const at::Tensor & other) {
  auto wrapper_Tensor_lt__tmp = wrapper_Tensor_lt(self, other);
  at::_copy_from(wrapper_Tensor_lt__tmp, self);
  return self;
}
namespace {

at::Tensor & wrapper_Scalar_masked_fill_(at::Tensor & self, const at::Tensor & mask, const at::Scalar & value) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::masked_fill_(self, mask, value);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Tensor_masked_fill_(at::Tensor & self, const at::Tensor & mask, const at::Tensor & value) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::masked_fill_(self, mask, value);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper_dim_max(const at::Tensor & self, int64_t dim, bool keepdim) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::max(self, dim, keepdim);
}

} // anonymous namespace
::std::tuple<at::Tensor &,at::Tensor &> wrapper_dim_max_max_out(const at::Tensor & self, int64_t dim, bool keepdim, at::Tensor & max, at::Tensor & max_values) {
  auto wrapper_dim_max_max_out_tmp = wrapper_dim_max(self, dim, keepdim);
  at::_copy_from_and_resize(std::get<0>(wrapper_dim_max_max_out_tmp), max);
  at::_copy_from_and_resize(std::get<1>(wrapper_dim_max_max_out_tmp), max_values);
  return ::std::tuple<at::Tensor &,at::Tensor &>(max, max_values);
}
namespace {

at::Tensor wrapper__max(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::max(self);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper__max_pool2d_with_indices(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::max_pool2d_with_indices(self, kernel_size, stride, padding, dilation, ceil_mode);
}

} // anonymous namespace
::std::tuple<at::Tensor &,at::Tensor &> wrapper_out_max_pool2d_with_indices_out(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, at::Tensor & out, at::Tensor & indices) {
  auto wrapper_out_max_pool2d_with_indices_out_tmp = wrapper__max_pool2d_with_indices(self, kernel_size, stride, padding, dilation, ceil_mode);
  at::_copy_from_and_resize(std::get<0>(wrapper_out_max_pool2d_with_indices_out_tmp), out);
  at::_copy_from_and_resize(std::get<1>(wrapper_out_max_pool2d_with_indices_out_tmp), indices);
  return ::std::tuple<at::Tensor &,at::Tensor &>(out, indices);
}
namespace {

at::Tensor wrapper__max_pool2d_with_indices_backward(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, const at::Tensor & indices) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::max_pool2d_with_indices_backward(grad_output, self, kernel_size, stride, padding, dilation, ceil_mode, indices);
}

} // anonymous namespace
at::Tensor & wrapper_grad_input_max_pool2d_with_indices_backward_out(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, const at::Tensor & indices, at::Tensor & grad_input) {
  auto wrapper_grad_input_max_pool2d_with_indices_backward_out_tmp = wrapper__max_pool2d_with_indices_backward(grad_output, self, kernel_size, stride, padding, dilation, ceil_mode, indices);
  at::_copy_from_and_resize(wrapper_grad_input_max_pool2d_with_indices_backward_out_tmp, grad_input);
  return grad_input;
}
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper__max_pool3d_with_indices(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::max_pool3d_with_indices(self, kernel_size, stride, padding, dilation, ceil_mode);
}

} // anonymous namespace
::std::tuple<at::Tensor &,at::Tensor &> wrapper_out_max_pool3d_with_indices_out(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, at::Tensor & out, at::Tensor & indices) {
  auto wrapper_out_max_pool3d_with_indices_out_tmp = wrapper__max_pool3d_with_indices(self, kernel_size, stride, padding, dilation, ceil_mode);
  at::_copy_from_and_resize(std::get<0>(wrapper_out_max_pool3d_with_indices_out_tmp), out);
  at::_copy_from_and_resize(std::get<1>(wrapper_out_max_pool3d_with_indices_out_tmp), indices);
  return ::std::tuple<at::Tensor &,at::Tensor &>(out, indices);
}
namespace {

at::Tensor wrapper__max_pool3d_with_indices_backward(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, const at::Tensor & indices) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::max_pool3d_with_indices_backward(grad_output, self, kernel_size, stride, padding, dilation, ceil_mode, indices);
}

} // anonymous namespace
at::Tensor & wrapper_grad_input_max_pool3d_with_indices_backward_out(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, const at::Tensor & indices, at::Tensor & grad_input) {
  auto wrapper_grad_input_max_pool3d_with_indices_backward_out_tmp = wrapper__max_pool3d_with_indices_backward(grad_output, self, kernel_size, stride, padding, dilation, ceil_mode, indices);
  at::_copy_from_and_resize(wrapper_grad_input_max_pool3d_with_indices_backward_out_tmp, grad_input);
  return grad_input;
}
namespace {

at::Tensor wrapper__maximum(const at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::maximum(self, other);
}

} // anonymous namespace
at::Tensor & wrapper_out_maximum_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto wrapper_out_maximum_out_tmp = wrapper__maximum(self, other);
  at::_copy_from_and_resize(wrapper_out_maximum_out_tmp, out);
  return out;
}
namespace {

at::Tensor wrapper__mean(const at::Tensor & self, c10::optional<at::ScalarType> dtype) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::mean(self, dtype);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_dim_mean(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::mean(self, dim, keepdim, dtype);
}

} // anonymous namespace
at::Tensor & wrapper_out_mean_out(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
  auto wrapper_out_mean_out_tmp = wrapper_dim_mean(self, dim, keepdim, dtype);
  at::_copy_from_and_resize(wrapper_out_mean_out_tmp, out);
  return out;
}
namespace {

at::Tensor wrapper__min(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::min(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__minimum(const at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::minimum(self, other);
}

} // anonymous namespace
at::Tensor & wrapper_out_minimum_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto wrapper_out_minimum_out_tmp = wrapper__minimum(self, other);
  at::_copy_from_and_resize(wrapper_out_minimum_out_tmp, out);
  return out;
}
namespace {

at::Tensor wrapper__mm(const at::Tensor & self, const at::Tensor & mat2) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::mm(self, mat2);
}

} // anonymous namespace
at::Tensor & wrapper_out_mm_out(const at::Tensor & self, const at::Tensor & mat2, at::Tensor & out) {
  auto wrapper_out_mm_out_tmp = wrapper__mm(self, mat2);
  at::_copy_from_and_resize(wrapper_out_mm_out_tmp, out);
  return out;
}
namespace {

at::Tensor wrapper_Tensor_mul(const at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::mul(self, other);
}

} // anonymous namespace
at::Tensor & wrapper_out_mul_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto wrapper_out_mul_out_tmp = wrapper_Tensor_mul(self, other);
  at::_copy_from_and_resize(wrapper_out_mul_out_tmp, out);
  return out;
}
at::Tensor & wrapper_Tensor_mul_(at::Tensor & self, const at::Tensor & other) {
  auto wrapper_Tensor_mul__tmp = wrapper_Tensor_mul(self, other);
  at::_copy_from(wrapper_Tensor_mul__tmp, self);
  return self;
}
namespace {

at::Tensor wrapper__mv(const at::Tensor & self, const at::Tensor & vec) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::mv(self, vec);
}

} // anonymous namespace
at::Tensor & wrapper_out_mv_out(const at::Tensor & self, const at::Tensor & vec, at::Tensor & out) {
  auto wrapper_out_mv_out_tmp = wrapper__mv(self, vec);
  at::_copy_from_and_resize(wrapper_out_mv_out_tmp, out);
  return out;
}
namespace {

::std::tuple<at::Tensor,at::Tensor,at::Tensor> wrapper__native_batch_norm(const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, bool training, double momentum, double eps) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::native_batch_norm(input, weight, bias, running_mean, running_var, training, momentum, eps);
}

} // anonymous namespace
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> wrapper_out_native_batch_norm_out(const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, bool training, double momentum, double eps, at::Tensor & out, at::Tensor & save_mean, at::Tensor & save_invstd) {
  auto wrapper_out_native_batch_norm_out_tmp = wrapper__native_batch_norm(input, weight, bias, running_mean, running_var, training, momentum, eps);
  at::_copy_from_and_resize(std::get<0>(wrapper_out_native_batch_norm_out_tmp), out);
  at::_copy_from_and_resize(std::get<1>(wrapper_out_native_batch_norm_out_tmp), save_mean);
  at::_copy_from_and_resize(std::get<2>(wrapper_out_native_batch_norm_out_tmp), save_invstd);
  return ::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out, save_mean, save_invstd);
}
namespace {

::std::tuple<at::Tensor,at::Tensor,at::Tensor> wrapper__native_batch_norm_backward(const at::Tensor & grad_out, const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, const c10::optional<at::Tensor> & save_mean, const c10::optional<at::Tensor> & save_invstd, bool train, double eps, ::std::array<bool,3> output_mask) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::native_batch_norm_backward(grad_out, input, weight, running_mean, running_var, save_mean, save_invstd, train, eps, output_mask);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper__native_dropout(const at::Tensor & input, double p, c10::optional<bool> train) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::native_dropout(input, p, train);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__native_dropout_backward(const at::Tensor & grad_output, const at::Tensor & mask, double scale) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::native_dropout_backward(grad_output, mask, scale);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor,at::Tensor> wrapper__native_layer_norm(const at::Tensor & input, at::IntArrayRef normalized_shape, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, double eps) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::native_layer_norm(input, normalized_shape, weight, bias, eps);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor,at::Tensor> wrapper__native_layer_norm_backward(const at::Tensor & grad_out, const at::Tensor & input, at::IntArrayRef normalized_shape, const at::Tensor & mean, const at::Tensor & rstd, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, ::std::array<bool,3> output_mask) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::native_layer_norm_backward(grad_out, input, normalized_shape, mean, rstd, weight, bias, output_mask);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Scalar_ne(const at::Tensor & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::ne(self, other);
}

} // anonymous namespace
at::Tensor & wrapper_Scalar_out_ne_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  auto wrapper_Scalar_out_ne_out_tmp = wrapper_Scalar_ne(self, other);
  at::_copy_from_and_resize(wrapper_Scalar_out_ne_out_tmp, out);
  return out;
}
at::Tensor & wrapper_Scalar_ne_(at::Tensor & self, const at::Scalar & other) {
  auto wrapper_Scalar_ne__tmp = wrapper_Scalar_ne(self, other);
  at::_copy_from(wrapper_Scalar_ne__tmp, self);
  return self;
}
namespace {

at::Tensor wrapper_Tensor_ne(const at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::ne(self, other);
}

} // anonymous namespace
at::Tensor & wrapper_Tensor_out_ne_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto wrapper_Tensor_out_ne_out_tmp = wrapper_Tensor_ne(self, other);
  at::_copy_from_and_resize(wrapper_Tensor_out_ne_out_tmp, out);
  return out;
}
at::Tensor & wrapper_Tensor_ne_(at::Tensor & self, const at::Tensor & other) {
  auto wrapper_Tensor_ne__tmp = wrapper_Tensor_ne(self, other);
  at::_copy_from(wrapper_Tensor_ne__tmp, self);
  return self;
}
namespace {

at::Tensor wrapper__neg(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::neg(self);
}

} // anonymous namespace
at::Tensor & wrapper_out_neg_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_out_neg_out_tmp = wrapper__neg(self);
  at::_copy_from_and_resize(wrapper_out_neg_out_tmp, out);
  return out;
}
at::Tensor & wrapper__neg_(at::Tensor & self) {
  auto wrapper__neg__tmp = wrapper__neg(self);
  at::_copy_from(wrapper__neg__tmp, self);
  return self;
}
namespace {

at::Tensor wrapper__nll_loss2d_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index, const at::Tensor & total_weight) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::nll_loss2d_backward(grad_output, self, target, weight, reduction, ignore_index, total_weight);
}

} // anonymous namespace
at::Tensor & wrapper_grad_input_nll_loss2d_backward_out(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index, const at::Tensor & total_weight, at::Tensor & grad_input) {
  auto wrapper_grad_input_nll_loss2d_backward_out_tmp = wrapper__nll_loss2d_backward(grad_output, self, target, weight, reduction, ignore_index, total_weight);
  at::_copy_from_and_resize(wrapper_grad_input_nll_loss2d_backward_out_tmp, grad_input);
  return grad_input;
}
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper__nll_loss2d_forward(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::nll_loss2d_forward(self, target, weight, reduction, ignore_index);
}

} // anonymous namespace
::std::tuple<at::Tensor &,at::Tensor &> wrapper_output_nll_loss2d_forward_out(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index, at::Tensor & output, at::Tensor & total_weight) {
  auto wrapper_output_nll_loss2d_forward_out_tmp = wrapper__nll_loss2d_forward(self, target, weight, reduction, ignore_index);
  at::_copy_from_and_resize(std::get<0>(wrapper_output_nll_loss2d_forward_out_tmp), output);
  at::_copy_from_and_resize(std::get<1>(wrapper_output_nll_loss2d_forward_out_tmp), total_weight);
  return ::std::tuple<at::Tensor &,at::Tensor &>(output, total_weight);
}
namespace {

at::Tensor wrapper__nll_loss_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index, const at::Tensor & total_weight) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::nll_loss_backward(grad_output, self, target, weight, reduction, ignore_index, total_weight);
}

} // anonymous namespace
at::Tensor & wrapper_grad_input_nll_loss_backward_out(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index, const at::Tensor & total_weight, at::Tensor & grad_input) {
  auto wrapper_grad_input_nll_loss_backward_out_tmp = wrapper__nll_loss_backward(grad_output, self, target, weight, reduction, ignore_index, total_weight);
  at::_copy_from_and_resize(wrapper_grad_input_nll_loss_backward_out_tmp, grad_input);
  return grad_input;
}
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper__nll_loss_forward(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::nll_loss_forward(self, target, weight, reduction, ignore_index);
}

} // anonymous namespace
::std::tuple<at::Tensor &,at::Tensor &> wrapper_output_nll_loss_forward_out(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index, at::Tensor & output, at::Tensor & total_weight) {
  auto wrapper_output_nll_loss_forward_out_tmp = wrapper__nll_loss_forward(self, target, weight, reduction, ignore_index);
  at::_copy_from_and_resize(std::get<0>(wrapper_output_nll_loss_forward_out_tmp), output);
  at::_copy_from_and_resize(std::get<1>(wrapper_output_nll_loss_forward_out_tmp), total_weight);
  return ::std::tuple<at::Tensor &,at::Tensor &>(output, total_weight);
}
namespace {

at::Tensor wrapper_ScalarOpt_dim_norm(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::IntArrayRef dim, bool keepdim) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::norm(self, p, dim, keepdim);
}

} // anonymous namespace
at::Tensor & wrapper_out_norm_out(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
  auto wrapper_out_norm_out_tmp = wrapper_ScalarOpt_dim_norm(self, p, dim, keepdim);
  at::_copy_from_and_resize(wrapper_out_norm_out_tmp, out);
  return out;
}
namespace {

at::Tensor & wrapper__normal_(at::Tensor & self, double mean, double std, c10::optional<at::Generator> generator) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::normal_(self, mean, std, generator);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__permute(const at::Tensor & self, at::IntArrayRef dims) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::permute(self, dims);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Tensor_Tensor_pow(const at::Tensor & self, const at::Tensor & exponent) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::pow(self, exponent);
}

} // anonymous namespace
at::Tensor & wrapper_Tensor_Tensor_out_pow_out(const at::Tensor & self, const at::Tensor & exponent, at::Tensor & out) {
  auto wrapper_Tensor_Tensor_out_pow_out_tmp = wrapper_Tensor_Tensor_pow(self, exponent);
  at::_copy_from_and_resize(wrapper_Tensor_Tensor_out_pow_out_tmp, out);
  return out;
}
at::Tensor & wrapper_Tensor_pow_(at::Tensor & self, const at::Tensor & exponent) {
  auto wrapper_Tensor_pow__tmp = wrapper_Tensor_Tensor_pow(self, exponent);
  at::_copy_from(wrapper_Tensor_pow__tmp, self);
  return self;
}
namespace {

at::Tensor wrapper_Tensor_Scalar_pow(const at::Tensor & self, const at::Scalar & exponent) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::pow(self, exponent);
}

} // anonymous namespace
at::Tensor & wrapper_Tensor_Scalar_out_pow_out(const at::Tensor & self, const at::Scalar & exponent, at::Tensor & out) {
  auto wrapper_Tensor_Scalar_out_pow_out_tmp = wrapper_Tensor_Scalar_pow(self, exponent);
  at::_copy_from_and_resize(wrapper_Tensor_Scalar_out_pow_out_tmp, out);
  return out;
}
at::Tensor & wrapper_Scalar_pow_(at::Tensor & self, const at::Scalar & exponent) {
  auto wrapper_Scalar_pow__tmp = wrapper_Tensor_Scalar_pow(self, exponent);
  at::_copy_from(wrapper_Scalar_pow__tmp, self);
  return self;
}
namespace {

at::Tensor & wrapper_from_random_(at::Tensor & self, int64_t from, c10::optional<int64_t> to, c10::optional<at::Generator> generator) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::random_(self, from, to, generator);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_to_random_(at::Tensor & self, int64_t to, c10::optional<at::Generator> generator) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::random_(self, to, generator);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__random_(at::Tensor & self, c10::optional<at::Generator> generator) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::random_(self, generator);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__reciprocal(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::reciprocal(self);
}

} // anonymous namespace
at::Tensor & wrapper_out_reciprocal_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_out_reciprocal_out_tmp = wrapper__reciprocal(self);
  at::_copy_from_and_resize(wrapper_out_reciprocal_out_tmp, out);
  return out;
}
at::Tensor & wrapper__reciprocal_(at::Tensor & self) {
  auto wrapper__reciprocal__tmp = wrapper__reciprocal(self);
  at::_copy_from(wrapper__reciprocal__tmp, self);
  return self;
}
namespace {

at::Tensor wrapper__relu(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::relu(self);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__relu_(at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::relu_(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Tensor_remainder(const at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::remainder(self, other);
}

} // anonymous namespace
at::Tensor & wrapper_Tensor_out_remainder_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  auto wrapper_Tensor_out_remainder_out_tmp = wrapper_Tensor_remainder(self, other);
  at::_copy_from_and_resize(wrapper_Tensor_out_remainder_out_tmp, out);
  return out;
}
at::Tensor & wrapper_Tensor_remainder_(at::Tensor & self, const at::Tensor & other) {
  auto wrapper_Tensor_remainder__tmp = wrapper_Tensor_remainder(self, other);
  at::_copy_from(wrapper_Tensor_remainder__tmp, self);
  return self;
}
namespace {

at::Tensor wrapper__repeat(const at::Tensor & self, at::IntArrayRef repeats) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::repeat(self, repeats);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__rsqrt(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::rsqrt(self);
}

} // anonymous namespace
at::Tensor & wrapper_out_rsqrt_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_out_rsqrt_out_tmp = wrapper__rsqrt(self);
  at::_copy_from_and_resize(wrapper_out_rsqrt_out_tmp, out);
  return out;
}
at::Tensor & wrapper__rsqrt_(at::Tensor & self) {
  auto wrapper__rsqrt__tmp = wrapper__rsqrt(self);
  at::_copy_from(wrapper__rsqrt__tmp, self);
  return self;
}
namespace {

at::Tensor wrapper__scatter_add(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::scatter_add(self, dim, index, src);
}

} // anonymous namespace
at::Tensor & wrapper_out_scatter_add_out(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src, at::Tensor & out) {
  auto wrapper_out_scatter_add_out_tmp = wrapper__scatter_add(self, dim, index, src);
  at::_copy_from_and_resize(wrapper_out_scatter_add_out_tmp, out);
  return out;
}
at::Tensor & wrapper__scatter_add_(at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src) {
  auto wrapper__scatter_add__tmp = wrapper__scatter_add(self, dim, index, src);
  at::_copy_from(wrapper__scatter_add__tmp, self);
  return self;
}
namespace {

at::Tensor wrapper_int_select(const at::Tensor & self, int64_t dim, int64_t index) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::select(self, dim, index);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__sgn(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::sgn(self);
}

} // anonymous namespace
at::Tensor & wrapper_out_sgn_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_out_sgn_out_tmp = wrapper__sgn(self);
  at::_copy_from_and_resize(wrapper_out_sgn_out_tmp, out);
  return out;
}
at::Tensor & wrapper__sgn_(at::Tensor & self) {
  auto wrapper__sgn__tmp = wrapper__sgn(self);
  at::_copy_from(wrapper__sgn__tmp, self);
  return self;
}
namespace {

at::Tensor wrapper__sigmoid(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::sigmoid(self);
}

} // anonymous namespace
at::Tensor & wrapper_out_sigmoid_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_out_sigmoid_out_tmp = wrapper__sigmoid(self);
  at::_copy_from_and_resize(wrapper_out_sigmoid_out_tmp, out);
  return out;
}
at::Tensor & wrapper__sigmoid_(at::Tensor & self) {
  auto wrapper__sigmoid__tmp = wrapper__sigmoid(self);
  at::_copy_from(wrapper__sigmoid__tmp, self);
  return self;
}
namespace {

at::Tensor wrapper__sigmoid_backward(const at::Tensor & grad_output, const at::Tensor & output) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::sigmoid_backward(grad_output, output);
}

} // anonymous namespace
at::Tensor & wrapper_grad_input_sigmoid_backward_out(const at::Tensor & grad_output, const at::Tensor & output, at::Tensor & grad_input) {
  auto wrapper_grad_input_sigmoid_backward_out_tmp = wrapper__sigmoid_backward(grad_output, output);
  at::_copy_from_and_resize(wrapper_grad_input_sigmoid_backward_out_tmp, grad_input);
  return grad_input;
}
namespace {

at::Tensor wrapper__silu(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::silu(self);
}

} // anonymous namespace
at::Tensor & wrapper_out_silu_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_out_silu_out_tmp = wrapper__silu(self);
  at::_copy_from_and_resize(wrapper_out_silu_out_tmp, out);
  return out;
}
at::Tensor & wrapper__silu_(at::Tensor & self) {
  auto wrapper__silu__tmp = wrapper__silu(self);
  at::_copy_from(wrapper__silu__tmp, self);
  return self;
}
namespace {

at::Tensor wrapper_Tensor_slice(const at::Tensor & self, int64_t dim, c10::optional<int64_t> start, c10::optional<int64_t> end, int64_t step) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::slice(self, dim, start, end, step);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__smooth_l1_loss(const at::Tensor & self, const at::Tensor & target, int64_t reduction, double beta) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::smooth_l1_loss(self, target, reduction, beta);
}

} // anonymous namespace
at::Tensor & wrapper_out_smooth_l1_loss_out(const at::Tensor & self, const at::Tensor & target, int64_t reduction, double beta, at::Tensor & out) {
  auto wrapper_out_smooth_l1_loss_out_tmp = wrapper__smooth_l1_loss(self, target, reduction, beta);
  at::_copy_from_and_resize(wrapper_out_smooth_l1_loss_out_tmp, out);
  return out;
}
namespace {

at::Tensor wrapper__smooth_l1_loss_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double beta) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::smooth_l1_loss_backward(grad_output, self, target, reduction, beta);
}

} // anonymous namespace
at::Tensor & wrapper_grad_input_smooth_l1_loss_backward_out(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double beta, at::Tensor & grad_input) {
  auto wrapper_grad_input_smooth_l1_loss_backward_out_tmp = wrapper__smooth_l1_loss_backward(grad_output, self, target, reduction, beta);
  at::_copy_from_and_resize(wrapper_grad_input_smooth_l1_loss_backward_out_tmp, grad_input);
  return grad_input;
}
namespace {

at::Tensor wrapper__softplus(const at::Tensor & self, const at::Scalar & beta, const at::Scalar & threshold) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::softplus(self, beta, threshold);
}

} // anonymous namespace
at::Tensor & wrapper_out_softplus_out(const at::Tensor & self, const at::Scalar & beta, const at::Scalar & threshold, at::Tensor & out) {
  auto wrapper_out_softplus_out_tmp = wrapper__softplus(self, beta, threshold);
  at::_copy_from_and_resize(wrapper_out_softplus_out_tmp, out);
  return out;
}
namespace {

at::Tensor wrapper__softplus_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & beta, const at::Scalar & threshold) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::softplus_backward(grad_output, self, beta, threshold);
}

} // anonymous namespace
at::Tensor & wrapper_grad_input_softplus_backward_out(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & beta, const at::Scalar & threshold, at::Tensor & grad_input) {
  auto wrapper_grad_input_softplus_backward_out_tmp = wrapper__softplus_backward(grad_output, self, beta, threshold);
  at::_copy_from_and_resize(wrapper_grad_input_softplus_backward_out_tmp, grad_input);
  return grad_input;
}
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper__sort(const at::Tensor & self, int64_t dim, bool descending) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::sort(self, dim, descending);
}

} // anonymous namespace
::std::tuple<at::Tensor &,at::Tensor &> wrapper_values_sort_out(const at::Tensor & self, int64_t dim, bool descending, at::Tensor & values, at::Tensor & indices) {
  auto wrapper_values_sort_out_tmp = wrapper__sort(self, dim, descending);
  at::_copy_from_and_resize(std::get<0>(wrapper_values_sort_out_tmp), values);
  at::_copy_from_and_resize(std::get<1>(wrapper_values_sort_out_tmp), indices);
  return ::std::tuple<at::Tensor &,at::Tensor &>(values, indices);
}
namespace {

at::Tensor wrapper__sqrt(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::sqrt(self);
}

} // anonymous namespace
at::Tensor & wrapper_out_sqrt_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_out_sqrt_out_tmp = wrapper__sqrt(self);
  at::_copy_from_and_resize(wrapper_out_sqrt_out_tmp, out);
  return out;
}
at::Tensor & wrapper__sqrt_(at::Tensor & self) {
  auto wrapper__sqrt__tmp = wrapper__sqrt(self);
  at::_copy_from(wrapper__sqrt__tmp, self);
  return self;
}
namespace {

at::Tensor wrapper__squeeze(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::squeeze(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_dim_squeeze(const at::Tensor & self, int64_t dim) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::squeeze(self, dim);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__squeeze_(at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::squeeze_(self);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_dim_squeeze_(at::Tensor & self, int64_t dim) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::squeeze_(self, dim);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__stack(at::TensorList tensors, int64_t dim) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::stack(tensors, dim);
}

} // anonymous namespace
at::Tensor & wrapper_out_stack_out(at::TensorList tensors, int64_t dim, at::Tensor & out) {
  auto wrapper_out_stack_out_tmp = wrapper__stack(tensors, dim);
  at::_copy_from_and_resize(wrapper_out_stack_out_tmp, out);
  return out;
}
namespace {

at::Tensor wrapper__std(const at::Tensor & self, bool unbiased) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::std(self, unbiased);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_dim_std(const at::Tensor & self, at::IntArrayRef dim, bool unbiased, bool keepdim) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::std(self, dim, unbiased, keepdim);
}

} // anonymous namespace
at::Tensor & wrapper_out_std_out(const at::Tensor & self, at::IntArrayRef dim, bool unbiased, bool keepdim, at::Tensor & out) {
  auto wrapper_out_std_out_tmp = wrapper_dim_std(self, dim, unbiased, keepdim);
  at::_copy_from_and_resize(wrapper_out_std_out_tmp, out);
  return out;
}
namespace {

at::Tensor wrapper_correction_std(const at::Tensor & self, c10::optional<at::IntArrayRef> dim, c10::optional<int64_t> correction, bool keepdim) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::std(self, dim, correction, keepdim);
}

} // anonymous namespace
at::Tensor & wrapper_correction_out_std_out(const at::Tensor & self, c10::optional<at::IntArrayRef> dim, c10::optional<int64_t> correction, bool keepdim, at::Tensor & out) {
  auto wrapper_correction_out_std_out_tmp = wrapper_correction_std(self, dim, correction, keepdim);
  at::_copy_from_and_resize(wrapper_correction_out_std_out_tmp, out);
  return out;
}
namespace {

at::Tensor wrapper_Tensor_sub(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::sub(self, other, alpha);
}

} // anonymous namespace
at::Tensor & wrapper_out_sub_out(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha, at::Tensor & out) {
  auto wrapper_out_sub_out_tmp = wrapper_Tensor_sub(self, other, alpha);
  at::_copy_from_and_resize(wrapper_out_sub_out_tmp, out);
  return out;
}
at::Tensor & wrapper_Tensor_sub_(at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
  auto wrapper_Tensor_sub__tmp = wrapper_Tensor_sub(self, other, alpha);
  at::_copy_from(wrapper_Tensor_sub__tmp, self);
  return self;
}
namespace {

at::Tensor wrapper__sum(const at::Tensor & self, c10::optional<at::ScalarType> dtype) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::sum(self, dtype);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_dim_IntList_sum(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::sum(self, dim, keepdim, dtype);
}

} // anonymous namespace
at::Tensor & wrapper_IntList_out_sum_out(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
  auto wrapper_IntList_out_sum_out_tmp = wrapper_dim_IntList_sum(self, dim, keepdim, dtype);
  at::_copy_from_and_resize(wrapper_IntList_out_sum_out_tmp, out);
  return out;
}
namespace {

at::Tensor wrapper__t(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::t(self);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__t_(at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::t_(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__tanh(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::tanh(self);
}

} // anonymous namespace
at::Tensor & wrapper_out_tanh_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_out_tanh_out_tmp = wrapper__tanh(self);
  at::_copy_from_and_resize(wrapper_out_tanh_out_tmp, out);
  return out;
}
at::Tensor & wrapper__tanh_(at::Tensor & self) {
  auto wrapper__tanh__tmp = wrapper__tanh(self);
  at::_copy_from(wrapper__tanh__tmp, self);
  return self;
}
namespace {

at::Tensor wrapper__tanh_backward(const at::Tensor & grad_output, const at::Tensor & output) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::tanh_backward(grad_output, output);
}

} // anonymous namespace
at::Tensor & wrapper_grad_input_tanh_backward_out(const at::Tensor & grad_output, const at::Tensor & output, at::Tensor & grad_input) {
  auto wrapper_grad_input_tanh_backward_out_tmp = wrapper__tanh_backward(grad_output, output);
  at::_copy_from_and_resize(wrapper_grad_input_tanh_backward_out_tmp, grad_input);
  return grad_input;
}
namespace {

at::Tensor wrapper__threshold(const at::Tensor & self, const at::Scalar & threshold, const at::Scalar & value) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::threshold(self, threshold, value);
}

} // anonymous namespace
at::Tensor & wrapper_out_threshold_out(const at::Tensor & self, const at::Scalar & threshold, const at::Scalar & value, at::Tensor & out) {
  auto wrapper_out_threshold_out_tmp = wrapper__threshold(self, threshold, value);
  at::_copy_from_and_resize(wrapper_out_threshold_out_tmp, out);
  return out;
}
at::Tensor & wrapper__threshold_(at::Tensor & self, const at::Scalar & threshold, const at::Scalar & value) {
  auto wrapper__threshold__tmp = wrapper__threshold(self, threshold, value);
  at::_copy_from(wrapper__threshold__tmp, self);
  return self;
}
namespace {

at::Tensor wrapper__threshold_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & threshold) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::threshold_backward(grad_output, self, threshold);
}

} // anonymous namespace
at::Tensor & wrapper_grad_input_threshold_backward_out(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & threshold, at::Tensor & grad_input) {
  auto wrapper_grad_input_threshold_backward_out_tmp = wrapper__threshold_backward(grad_output, self, threshold);
  at::_copy_from_and_resize(wrapper_grad_input_threshold_backward_out_tmp, grad_input);
  return grad_input;
}
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper__topk(const at::Tensor & self, int64_t k, int64_t dim, bool largest, bool sorted) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::topk(self, k, dim, largest, sorted);
}

} // anonymous namespace
::std::tuple<at::Tensor &,at::Tensor &> wrapper_values_topk_out(const at::Tensor & self, int64_t k, int64_t dim, bool largest, bool sorted, at::Tensor & values, at::Tensor & indices) {
  auto wrapper_values_topk_out_tmp = wrapper__topk(self, k, dim, largest, sorted);
  at::_copy_from_and_resize(std::get<0>(wrapper_values_topk_out_tmp), values);
  at::_copy_from_and_resize(std::get<1>(wrapper_values_topk_out_tmp), indices);
  return ::std::tuple<at::Tensor &,at::Tensor &>(values, indices);
}
namespace {

at::Tensor wrapper__trace(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::trace(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_int_transpose(const at::Tensor & self, int64_t dim0, int64_t dim1) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::transpose(self, dim0, dim1);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__transpose_(at::Tensor & self, int64_t dim0, int64_t dim1) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::transpose_(self, dim0, dim1);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__tril(const at::Tensor & self, int64_t diagonal) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::tril(self, diagonal);
}

} // anonymous namespace
at::Tensor & wrapper_out_tril_out(const at::Tensor & self, int64_t diagonal, at::Tensor & out) {
  auto wrapper_out_tril_out_tmp = wrapper__tril(self, diagonal);
  at::_copy_from_and_resize(wrapper_out_tril_out_tmp, out);
  return out;
}
at::Tensor & wrapper__tril_(at::Tensor & self, int64_t diagonal) {
  auto wrapper__tril__tmp = wrapper__tril(self, diagonal);
  at::_copy_from(wrapper__tril__tmp, self);
  return self;
}
namespace {

at::Tensor wrapper__triu(const at::Tensor & self, int64_t diagonal) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::triu(self, diagonal);
}

} // anonymous namespace
at::Tensor & wrapper_out_triu_out(const at::Tensor & self, int64_t diagonal, at::Tensor & out) {
  auto wrapper_out_triu_out_tmp = wrapper__triu(self, diagonal);
  at::_copy_from_and_resize(wrapper_out_triu_out_tmp, out);
  return out;
}
at::Tensor & wrapper__triu_(at::Tensor & self, int64_t diagonal) {
  auto wrapper__triu__tmp = wrapper__triu(self, diagonal);
  at::_copy_from(wrapper__triu__tmp, self);
  return self;
}
namespace {

at::Tensor wrapper__trunc(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::trunc(self);
}

} // anonymous namespace
at::Tensor & wrapper_out_trunc_out(const at::Tensor & self, at::Tensor & out) {
  auto wrapper_out_trunc_out_tmp = wrapper__trunc(self);
  at::_copy_from_and_resize(wrapper_out_trunc_out_tmp, out);
  return out;
}
at::Tensor & wrapper__trunc_(at::Tensor & self) {
  auto wrapper__trunc__tmp = wrapper__trunc(self);
  at::_copy_from(wrapper__trunc__tmp, self);
  return self;
}
namespace {

at::Tensor wrapper__unsqueeze(const at::Tensor & self, int64_t dim) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::unsqueeze(self, dim);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__unsqueeze_(at::Tensor & self, int64_t dim) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::unsqueeze_(self, dim);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__upsample_bilinear2d(const at::Tensor & self, at::IntArrayRef output_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::upsample_bilinear2d(self, output_size, align_corners, scales_h, scales_w);
}

} // anonymous namespace
at::Tensor & wrapper_out_upsample_bilinear2d_out(const at::Tensor & self, at::IntArrayRef output_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & out) {
  auto wrapper_out_upsample_bilinear2d_out_tmp = wrapper__upsample_bilinear2d(self, output_size, align_corners, scales_h, scales_w);
  at::_copy_from_and_resize(wrapper_out_upsample_bilinear2d_out_tmp, out);
  return out;
}
namespace {

at::Tensor wrapper__upsample_bilinear2d_backward(const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::upsample_bilinear2d_backward(grad_output, output_size, input_size, align_corners, scales_h, scales_w);
}

} // anonymous namespace
at::Tensor & wrapper_grad_input_upsample_bilinear2d_backward_out(const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & grad_input) {
  auto wrapper_grad_input_upsample_bilinear2d_backward_out_tmp = wrapper__upsample_bilinear2d_backward(grad_output, output_size, input_size, align_corners, scales_h, scales_w);
  at::_copy_from_and_resize(wrapper_grad_input_upsample_bilinear2d_backward_out_tmp, grad_input);
  return grad_input;
}
namespace {

at::Tensor wrapper__upsample_nearest2d(const at::Tensor & self, at::IntArrayRef output_size, c10::optional<double> scales_h, c10::optional<double> scales_w) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::upsample_nearest2d(self, output_size, scales_h, scales_w);
}

} // anonymous namespace
at::Tensor & wrapper_out_upsample_nearest2d_out(const at::Tensor & self, at::IntArrayRef output_size, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & out) {
  auto wrapper_out_upsample_nearest2d_out_tmp = wrapper__upsample_nearest2d(self, output_size, scales_h, scales_w);
  at::_copy_from_and_resize(wrapper_out_upsample_nearest2d_out_tmp, out);
  return out;
}
namespace {

at::Tensor wrapper__upsample_nearest2d_backward(const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, c10::optional<double> scales_h, c10::optional<double> scales_w) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::upsample_nearest2d_backward(grad_output, output_size, input_size, scales_h, scales_w);
}

} // anonymous namespace
at::Tensor & wrapper_grad_input_upsample_nearest2d_backward_out(const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & grad_input) {
  auto wrapper_grad_input_upsample_nearest2d_backward_out_tmp = wrapper__upsample_nearest2d_backward(grad_output, output_size, input_size, scales_h, scales_w);
  at::_copy_from_and_resize(wrapper_grad_input_upsample_nearest2d_backward_out_tmp, grad_input);
  return grad_input;
}
namespace {

at::Tensor wrapper__view(const at::Tensor & self, at::IntArrayRef size) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::view(self, size);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__zero_(at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return torch::lazy::LazyNativeFunctions::zero_(self);
}

} // anonymous namespace

TORCH_LIBRARY_IMPL(aten, Lazy, m) {
  m.impl("_adaptive_avg_pool2d",
  TORCH_FN(wrapper___adaptive_avg_pool2d));
  m.impl("_adaptive_avg_pool2d_backward",
  TORCH_FN(wrapper___adaptive_avg_pool2d_backward));
  m.impl("_copy_from",
  TORCH_FN(wrapper___copy_from));
  m.impl("_copy_from_and_resize",
  TORCH_FN(wrapper___copy_from_and_resize));
  m.impl("_log_softmax",
  TORCH_FN(wrapper___log_softmax));
  m.impl("_log_softmax.out",
  TORCH_FN(wrapper_out__log_softmax_out));
  m.impl("_log_softmax_backward_data",
  TORCH_FN(wrapper___log_softmax_backward_data));
  m.impl("_log_softmax_backward_data.out",
  TORCH_FN(wrapper_out__log_softmax_backward_data_out));
  m.impl("_softmax",
  TORCH_FN(wrapper___softmax));
  m.impl("_softmax.out",
  TORCH_FN(wrapper_out__softmax_out));
  m.impl("_softmax_backward_data",
  TORCH_FN(wrapper___softmax_backward_data));
  m.impl("_softmax_backward_data.out",
  TORCH_FN(wrapper_out__softmax_backward_data_out));
  m.impl("_to_copy",
  TORCH_FN(wrapper___to_copy));
  m.impl("_unsafe_view",
  TORCH_FN(wrapper___unsafe_view));
  m.impl("abs",
  TORCH_FN(wrapper__abs));
  m.impl("abs.out",
  TORCH_FN(wrapper_out_abs_out));
  m.impl("abs_",
  TORCH_FN(wrapper__abs_));
  m.impl("add.Tensor",
  TORCH_FN(wrapper_Tensor_add));
  m.impl("add.out",
  TORCH_FN(wrapper_out_add_out));
  m.impl("add_.Tensor",
  TORCH_FN(wrapper_Tensor_add_));
  m.impl("addcdiv",
  TORCH_FN(wrapper__addcdiv));
  m.impl("addcdiv.out",
  TORCH_FN(wrapper_out_addcdiv_out));
  m.impl("addcdiv_",
  TORCH_FN(wrapper__addcdiv_));
  m.impl("addcmul",
  TORCH_FN(wrapper__addcmul));
  m.impl("addcmul.out",
  TORCH_FN(wrapper_out_addcmul_out));
  m.impl("addcmul_",
  TORCH_FN(wrapper__addcmul_));
  m.impl("addmm",
  TORCH_FN(wrapper__addmm));
  m.impl("addmm.out",
  TORCH_FN(wrapper_out_addmm_out));
  m.impl("addmm_",
  TORCH_FN(wrapper__addmm_));
  m.impl("alias",
  TORCH_FN(wrapper__alias));
  m.impl("all",
  TORCH_FN(wrapper__all));
  m.impl("all.all_out",
  TORCH_FN(wrapper_all_out_all_out));
  m.impl("any",
  TORCH_FN(wrapper__any));
  m.impl("any.all_out",
  TORCH_FN(wrapper_all_out_any_out));
  m.impl("arange.start_out",
  TORCH_FN(wrapper_start_out_arange_out));
  m.impl("as_strided",
  TORCH_FN(wrapper__as_strided));
  m.impl("as_strided_",
  TORCH_FN(wrapper__as_strided_));
  m.impl("avg_pool2d",
  TORCH_FN(wrapper__avg_pool2d));
  m.impl("avg_pool2d.out",
  TORCH_FN(wrapper_out_avg_pool2d_out));
  m.impl("avg_pool2d_backward",
  TORCH_FN(wrapper__avg_pool2d_backward));
  m.impl("avg_pool2d_backward.grad_input",
  TORCH_FN(wrapper_grad_input_avg_pool2d_backward_out));
  m.impl("baddbmm",
  TORCH_FN(wrapper__baddbmm));
  m.impl("baddbmm.out",
  TORCH_FN(wrapper_out_baddbmm_out));
  m.impl("baddbmm_",
  TORCH_FN(wrapper__baddbmm_));
  m.impl("bernoulli",
  TORCH_FN(wrapper__bernoulli));
  m.impl("bernoulli.out",
  TORCH_FN(wrapper_out_bernoulli_out));
  m.impl("bernoulli_.float",
  TORCH_FN(wrapper_float_bernoulli_));
  m.impl("binary_cross_entropy",
  TORCH_FN(wrapper__binary_cross_entropy));
  m.impl("binary_cross_entropy.out",
  TORCH_FN(wrapper_out_binary_cross_entropy_out));
  m.impl("binary_cross_entropy_backward",
  TORCH_FN(wrapper__binary_cross_entropy_backward));
  m.impl("binary_cross_entropy_backward.grad_input",
  TORCH_FN(wrapper_grad_input_binary_cross_entropy_backward_out));
  m.impl("bitwise_and.Tensor",
  TORCH_FN(wrapper_Tensor_bitwise_and));
  m.impl("bitwise_and.Tensor_out",
  TORCH_FN(wrapper_Tensor_out_bitwise_and_out));
  m.impl("bitwise_and_.Tensor",
  TORCH_FN(wrapper_Tensor_bitwise_and_));
  m.impl("bitwise_or.Tensor",
  TORCH_FN(wrapper_Tensor_bitwise_or));
  m.impl("bitwise_or.Tensor_out",
  TORCH_FN(wrapper_Tensor_out_bitwise_or_out));
  m.impl("bitwise_or_.Tensor",
  TORCH_FN(wrapper_Tensor_bitwise_or_));
  m.impl("bmm",
  TORCH_FN(wrapper__bmm));
  m.impl("bmm.out",
  TORCH_FN(wrapper_out_bmm_out));
  m.impl("cat",
  TORCH_FN(wrapper__cat));
  m.impl("cat.out",
  TORCH_FN(wrapper_out_cat_out));
  m.impl("clamp",
  TORCH_FN(wrapper__clamp));
  m.impl("clamp.out",
  TORCH_FN(wrapper_out_clamp_out));
  m.impl("clamp_",
  TORCH_FN(wrapper__clamp_));
  m.impl("clamp_min",
  TORCH_FN(wrapper__clamp_min));
  m.impl("clamp_min.out",
  TORCH_FN(wrapper_out_clamp_min_out));
  m.impl("clamp_min_",
  TORCH_FN(wrapper__clamp_min_));
  m.impl("clone",
  TORCH_FN(wrapper__clone));
  m.impl("constant_pad_nd",
  TORCH_FN(wrapper__constant_pad_nd));
  m.impl("convolution",
  TORCH_FN(wrapper__convolution));
  m.impl("convolution_backward",
  TORCH_FN(wrapper__convolution_backward));
  m.impl("cos",
  TORCH_FN(wrapper__cos));
  m.impl("cos.out",
  TORCH_FN(wrapper_out_cos_out));
  m.impl("cos_",
  TORCH_FN(wrapper__cos_));
  m.impl("cumsum",
  TORCH_FN(wrapper__cumsum));
  m.impl("cumsum.out",
  TORCH_FN(wrapper_out_cumsum_out));
  m.impl("cumsum_",
  TORCH_FN(wrapper__cumsum_));
  m.impl("div.Tensor",
  TORCH_FN(wrapper_Tensor_div));
  m.impl("div.out",
  TORCH_FN(wrapper_out_div_out));
  m.impl("div_.Tensor",
  TORCH_FN(wrapper_Tensor_div_));
  m.impl("div.Tensor_mode",
  TORCH_FN(wrapper_Tensor_mode_div));
  m.impl("div.out_mode",
  TORCH_FN(wrapper_out_mode_div_out));
  m.impl("div_.Tensor_mode",
  TORCH_FN(wrapper_Tensor_mode_div_));
  m.impl("elu",
  TORCH_FN(wrapper__elu));
  m.impl("elu.out",
  TORCH_FN(wrapper_out_elu_out));
  m.impl("elu_",
  TORCH_FN(wrapper__elu_));
  m.impl("elu_backward",
  TORCH_FN(wrapper__elu_backward));
  m.impl("elu_backward.grad_input",
  TORCH_FN(wrapper_grad_input_elu_backward_out));
  m.impl("embedding",
  TORCH_FN(wrapper__embedding));
  m.impl("embedding_dense_backward",
  TORCH_FN(wrapper__embedding_dense_backward));
  m.impl("empty.memory_format",
  TORCH_FN(wrapper_memory_format_empty));
  m.impl("empty_strided",
  TORCH_FN(wrapper__empty_strided));
  m.impl("eq.Scalar",
  TORCH_FN(wrapper_Scalar_eq));
  m.impl("eq.Scalar_out",
  TORCH_FN(wrapper_Scalar_out_eq_out));
  m.impl("eq_.Scalar",
  TORCH_FN(wrapper_Scalar_eq_));
  m.impl("eq.Tensor",
  TORCH_FN(wrapper_Tensor_eq));
  m.impl("eq.Tensor_out",
  TORCH_FN(wrapper_Tensor_out_eq_out));
  m.impl("eq_.Tensor",
  TORCH_FN(wrapper_Tensor_eq_));
  m.impl("exp",
  TORCH_FN(wrapper__exp));
  m.impl("exp.out",
  TORCH_FN(wrapper_out_exp_out));
  m.impl("exp_",
  TORCH_FN(wrapper__exp_));
  m.impl("expand",
  TORCH_FN(wrapper__expand));
  m.impl("fill_.Scalar",
  TORCH_FN(wrapper_Scalar_fill_));
  m.impl("flip",
  TORCH_FN(wrapper__flip));
  m.impl("floor",
  TORCH_FN(wrapper__floor));
  m.impl("floor.out",
  TORCH_FN(wrapper_out_floor_out));
  m.impl("floor_",
  TORCH_FN(wrapper__floor_));
  m.impl("frac",
  TORCH_FN(wrapper__frac));
  m.impl("frac.out",
  TORCH_FN(wrapper_out_frac_out));
  m.impl("frac_",
  TORCH_FN(wrapper__frac_));
  m.impl("gather",
  TORCH_FN(wrapper__gather));
  m.impl("gather.out",
  TORCH_FN(wrapper_out_gather_out));
  m.impl("ge.Scalar",
  TORCH_FN(wrapper_Scalar_ge));
  m.impl("ge.Scalar_out",
  TORCH_FN(wrapper_Scalar_out_ge_out));
  m.impl("ge_.Scalar",
  TORCH_FN(wrapper_Scalar_ge_));
  m.impl("ge.Tensor",
  TORCH_FN(wrapper_Tensor_ge));
  m.impl("ge.Tensor_out",
  TORCH_FN(wrapper_Tensor_out_ge_out));
  m.impl("ge_.Tensor",
  TORCH_FN(wrapper_Tensor_ge_));
  m.impl("gelu",
  TORCH_FN(wrapper__gelu));
  m.impl("gelu.out",
  TORCH_FN(wrapper_out_gelu_out));
  m.impl("gelu_backward",
  TORCH_FN(wrapper__gelu_backward));
  m.impl("gelu_backward.grad_input",
  TORCH_FN(wrapper_grad_input_gelu_backward_out));
  m.impl("glu",
  TORCH_FN(wrapper__glu));
  m.impl("glu.out",
  TORCH_FN(wrapper_out_glu_out));
  m.impl("glu_backward",
  TORCH_FN(wrapper__glu_backward));
  m.impl("glu_backward.grad_input",
  TORCH_FN(wrapper_grad_input_glu_backward_out));
  m.impl("grid_sampler_2d",
  TORCH_FN(wrapper__grid_sampler_2d));
  m.impl("grid_sampler_2d_backward",
  TORCH_FN(wrapper__grid_sampler_2d_backward));
  m.impl("gt.Scalar",
  TORCH_FN(wrapper_Scalar_gt));
  m.impl("gt.Scalar_out",
  TORCH_FN(wrapper_Scalar_out_gt_out));
  m.impl("gt_.Scalar",
  TORCH_FN(wrapper_Scalar_gt_));
  m.impl("gt.Tensor",
  TORCH_FN(wrapper_Tensor_gt));
  m.impl("gt.Tensor_out",
  TORCH_FN(wrapper_Tensor_out_gt_out));
  m.impl("gt_.Tensor",
  TORCH_FN(wrapper_Tensor_gt_));
  m.impl("hardsigmoid",
  TORCH_FN(wrapper__hardsigmoid));
  m.impl("hardsigmoid.out",
  TORCH_FN(wrapper_out_hardsigmoid_out));
  m.impl("hardsigmoid_",
  TORCH_FN(wrapper__hardsigmoid_));
  m.impl("index_select",
  TORCH_FN(wrapper__index_select));
  m.impl("index_select.out",
  TORCH_FN(wrapper_out_index_select_out));
  m.impl("kl_div_backward",
  TORCH_FN(wrapper__kl_div_backward));
  m.impl("l1_loss_backward",
  TORCH_FN(wrapper__l1_loss_backward));
  m.impl("l1_loss_backward.grad_input",
  TORCH_FN(wrapper_grad_input_l1_loss_backward_out));
  m.impl("le.Scalar",
  TORCH_FN(wrapper_Scalar_le));
  m.impl("le.Scalar_out",
  TORCH_FN(wrapper_Scalar_out_le_out));
  m.impl("le_.Scalar",
  TORCH_FN(wrapper_Scalar_le_));
  m.impl("le.Tensor",
  TORCH_FN(wrapper_Tensor_le));
  m.impl("le.Tensor_out",
  TORCH_FN(wrapper_Tensor_out_le_out));
  m.impl("le_.Tensor",
  TORCH_FN(wrapper_Tensor_le_));
  m.impl("leaky_relu",
  TORCH_FN(wrapper__leaky_relu));
  m.impl("leaky_relu.out",
  TORCH_FN(wrapper_out_leaky_relu_out));
  m.impl("leaky_relu_",
  TORCH_FN(wrapper__leaky_relu_));
  m.impl("leaky_relu_backward",
  TORCH_FN(wrapper__leaky_relu_backward));
  m.impl("leaky_relu_backward.grad_input",
  TORCH_FN(wrapper_grad_input_leaky_relu_backward_out));
  m.impl("log",
  TORCH_FN(wrapper__log));
  m.impl("log.out",
  TORCH_FN(wrapper_out_log_out));
  m.impl("log_",
  TORCH_FN(wrapper__log_));
  m.impl("log2",
  TORCH_FN(wrapper__log2));
  m.impl("log2.out",
  TORCH_FN(wrapper_out_log2_out));
  m.impl("log2_",
  TORCH_FN(wrapper__log2_));
  m.impl("log_sigmoid_backward",
  TORCH_FN(wrapper__log_sigmoid_backward));
  m.impl("log_sigmoid_backward.grad_input",
  TORCH_FN(wrapper_grad_input_log_sigmoid_backward_out));
  m.impl("log_sigmoid_forward",
  TORCH_FN(wrapper__log_sigmoid_forward));
  m.impl("log_sigmoid_forward.output",
  TORCH_FN(wrapper_output_log_sigmoid_forward_out));
  m.impl("logdet",
  TORCH_FN(wrapper__logdet));
  m.impl("lt.Scalar",
  TORCH_FN(wrapper_Scalar_lt));
  m.impl("lt.Scalar_out",
  TORCH_FN(wrapper_Scalar_out_lt_out));
  m.impl("lt_.Scalar",
  TORCH_FN(wrapper_Scalar_lt_));
  m.impl("lt.Tensor",
  TORCH_FN(wrapper_Tensor_lt));
  m.impl("lt.Tensor_out",
  TORCH_FN(wrapper_Tensor_out_lt_out));
  m.impl("lt_.Tensor",
  TORCH_FN(wrapper_Tensor_lt_));
  m.impl("masked_fill_.Scalar",
  TORCH_FN(wrapper_Scalar_masked_fill_));
  m.impl("masked_fill_.Tensor",
  TORCH_FN(wrapper_Tensor_masked_fill_));
  m.impl("max.dim",
  TORCH_FN(wrapper_dim_max));
  m.impl("max.dim_max",
  TORCH_FN(wrapper_dim_max_max_out));
  m.impl("max",
  TORCH_FN(wrapper__max));
  m.impl("max_pool2d_with_indices",
  TORCH_FN(wrapper__max_pool2d_with_indices));
  m.impl("max_pool2d_with_indices.out",
  TORCH_FN(wrapper_out_max_pool2d_with_indices_out));
  m.impl("max_pool2d_with_indices_backward",
  TORCH_FN(wrapper__max_pool2d_with_indices_backward));
  m.impl("max_pool2d_with_indices_backward.grad_input",
  TORCH_FN(wrapper_grad_input_max_pool2d_with_indices_backward_out));
  m.impl("max_pool3d_with_indices",
  TORCH_FN(wrapper__max_pool3d_with_indices));
  m.impl("max_pool3d_with_indices.out",
  TORCH_FN(wrapper_out_max_pool3d_with_indices_out));
  m.impl("max_pool3d_with_indices_backward",
  TORCH_FN(wrapper__max_pool3d_with_indices_backward));
  m.impl("max_pool3d_with_indices_backward.grad_input",
  TORCH_FN(wrapper_grad_input_max_pool3d_with_indices_backward_out));
  m.impl("maximum",
  TORCH_FN(wrapper__maximum));
  m.impl("maximum.out",
  TORCH_FN(wrapper_out_maximum_out));
  m.impl("mean",
  TORCH_FN(wrapper__mean));
  m.impl("mean.dim",
  TORCH_FN(wrapper_dim_mean));
  m.impl("mean.out",
  TORCH_FN(wrapper_out_mean_out));
  m.impl("min",
  TORCH_FN(wrapper__min));
  m.impl("minimum",
  TORCH_FN(wrapper__minimum));
  m.impl("minimum.out",
  TORCH_FN(wrapper_out_minimum_out));
  m.impl("mm",
  TORCH_FN(wrapper__mm));
  m.impl("mm.out",
  TORCH_FN(wrapper_out_mm_out));
  m.impl("mul.Tensor",
  TORCH_FN(wrapper_Tensor_mul));
  m.impl("mul.out",
  TORCH_FN(wrapper_out_mul_out));
  m.impl("mul_.Tensor",
  TORCH_FN(wrapper_Tensor_mul_));
  m.impl("mv",
  TORCH_FN(wrapper__mv));
  m.impl("mv.out",
  TORCH_FN(wrapper_out_mv_out));
  m.impl("native_batch_norm",
  TORCH_FN(wrapper__native_batch_norm));
  m.impl("native_batch_norm.out",
  TORCH_FN(wrapper_out_native_batch_norm_out));
  m.impl("native_batch_norm_backward",
  TORCH_FN(wrapper__native_batch_norm_backward));
  m.impl("native_dropout",
  TORCH_FN(wrapper__native_dropout));
  m.impl("native_dropout_backward",
  TORCH_FN(wrapper__native_dropout_backward));
  m.impl("native_layer_norm",
  TORCH_FN(wrapper__native_layer_norm));
  m.impl("native_layer_norm_backward",
  TORCH_FN(wrapper__native_layer_norm_backward));
  m.impl("ne.Scalar",
  TORCH_FN(wrapper_Scalar_ne));
  m.impl("ne.Scalar_out",
  TORCH_FN(wrapper_Scalar_out_ne_out));
  m.impl("ne_.Scalar",
  TORCH_FN(wrapper_Scalar_ne_));
  m.impl("ne.Tensor",
  TORCH_FN(wrapper_Tensor_ne));
  m.impl("ne.Tensor_out",
  TORCH_FN(wrapper_Tensor_out_ne_out));
  m.impl("ne_.Tensor",
  TORCH_FN(wrapper_Tensor_ne_));
  m.impl("neg",
  TORCH_FN(wrapper__neg));
  m.impl("neg.out",
  TORCH_FN(wrapper_out_neg_out));
  m.impl("neg_",
  TORCH_FN(wrapper__neg_));
  m.impl("nll_loss2d_backward",
  TORCH_FN(wrapper__nll_loss2d_backward));
  m.impl("nll_loss2d_backward.grad_input",
  TORCH_FN(wrapper_grad_input_nll_loss2d_backward_out));
  m.impl("nll_loss2d_forward",
  TORCH_FN(wrapper__nll_loss2d_forward));
  m.impl("nll_loss2d_forward.output",
  TORCH_FN(wrapper_output_nll_loss2d_forward_out));
  m.impl("nll_loss_backward",
  TORCH_FN(wrapper__nll_loss_backward));
  m.impl("nll_loss_backward.grad_input",
  TORCH_FN(wrapper_grad_input_nll_loss_backward_out));
  m.impl("nll_loss_forward",
  TORCH_FN(wrapper__nll_loss_forward));
  m.impl("nll_loss_forward.output",
  TORCH_FN(wrapper_output_nll_loss_forward_out));
  m.impl("norm.ScalarOpt_dim",
  TORCH_FN(wrapper_ScalarOpt_dim_norm));
  m.impl("norm.out",
  TORCH_FN(wrapper_out_norm_out));
  m.impl("normal_",
  TORCH_FN(wrapper__normal_));
  m.impl("permute",
  TORCH_FN(wrapper__permute));
  m.impl("pow.Tensor_Tensor",
  TORCH_FN(wrapper_Tensor_Tensor_pow));
  m.impl("pow.Tensor_Tensor_out",
  TORCH_FN(wrapper_Tensor_Tensor_out_pow_out));
  m.impl("pow_.Tensor",
  TORCH_FN(wrapper_Tensor_pow_));
  m.impl("pow.Tensor_Scalar",
  TORCH_FN(wrapper_Tensor_Scalar_pow));
  m.impl("pow.Tensor_Scalar_out",
  TORCH_FN(wrapper_Tensor_Scalar_out_pow_out));
  m.impl("pow_.Scalar",
  TORCH_FN(wrapper_Scalar_pow_));
  m.impl("random_.from",
  TORCH_FN(wrapper_from_random_));
  m.impl("random_.to",
  TORCH_FN(wrapper_to_random_));
  m.impl("random_",
  TORCH_FN(wrapper__random_));
  m.impl("reciprocal",
  TORCH_FN(wrapper__reciprocal));
  m.impl("reciprocal.out",
  TORCH_FN(wrapper_out_reciprocal_out));
  m.impl("reciprocal_",
  TORCH_FN(wrapper__reciprocal_));
  m.impl("relu",
  TORCH_FN(wrapper__relu));
  m.impl("relu_",
  TORCH_FN(wrapper__relu_));
  m.impl("remainder.Tensor",
  TORCH_FN(wrapper_Tensor_remainder));
  m.impl("remainder.Tensor_out",
  TORCH_FN(wrapper_Tensor_out_remainder_out));
  m.impl("remainder_.Tensor",
  TORCH_FN(wrapper_Tensor_remainder_));
  m.impl("repeat",
  TORCH_FN(wrapper__repeat));
  m.impl("rsqrt",
  TORCH_FN(wrapper__rsqrt));
  m.impl("rsqrt.out",
  TORCH_FN(wrapper_out_rsqrt_out));
  m.impl("rsqrt_",
  TORCH_FN(wrapper__rsqrt_));
  m.impl("scatter_add",
  TORCH_FN(wrapper__scatter_add));
  m.impl("scatter_add.out",
  TORCH_FN(wrapper_out_scatter_add_out));
  m.impl("scatter_add_",
  TORCH_FN(wrapper__scatter_add_));
  m.impl("select.int",
  TORCH_FN(wrapper_int_select));
  m.impl("sgn",
  TORCH_FN(wrapper__sgn));
  m.impl("sgn.out",
  TORCH_FN(wrapper_out_sgn_out));
  m.impl("sgn_",
  TORCH_FN(wrapper__sgn_));
  m.impl("sigmoid",
  TORCH_FN(wrapper__sigmoid));
  m.impl("sigmoid.out",
  TORCH_FN(wrapper_out_sigmoid_out));
  m.impl("sigmoid_",
  TORCH_FN(wrapper__sigmoid_));
  m.impl("sigmoid_backward",
  TORCH_FN(wrapper__sigmoid_backward));
  m.impl("sigmoid_backward.grad_input",
  TORCH_FN(wrapper_grad_input_sigmoid_backward_out));
  m.impl("silu",
  TORCH_FN(wrapper__silu));
  m.impl("silu.out",
  TORCH_FN(wrapper_out_silu_out));
  m.impl("silu_",
  TORCH_FN(wrapper__silu_));
  m.impl("slice.Tensor",
  TORCH_FN(wrapper_Tensor_slice));
  m.impl("smooth_l1_loss",
  TORCH_FN(wrapper__smooth_l1_loss));
  m.impl("smooth_l1_loss.out",
  TORCH_FN(wrapper_out_smooth_l1_loss_out));
  m.impl("smooth_l1_loss_backward",
  TORCH_FN(wrapper__smooth_l1_loss_backward));
  m.impl("smooth_l1_loss_backward.grad_input",
  TORCH_FN(wrapper_grad_input_smooth_l1_loss_backward_out));
  m.impl("softplus",
  TORCH_FN(wrapper__softplus));
  m.impl("softplus.out",
  TORCH_FN(wrapper_out_softplus_out));
  m.impl("softplus_backward",
  TORCH_FN(wrapper__softplus_backward));
  m.impl("softplus_backward.grad_input",
  TORCH_FN(wrapper_grad_input_softplus_backward_out));
  m.impl("sort",
  TORCH_FN(wrapper__sort));
  m.impl("sort.values",
  TORCH_FN(wrapper_values_sort_out));
  m.impl("sqrt",
  TORCH_FN(wrapper__sqrt));
  m.impl("sqrt.out",
  TORCH_FN(wrapper_out_sqrt_out));
  m.impl("sqrt_",
  TORCH_FN(wrapper__sqrt_));
  m.impl("squeeze",
  TORCH_FN(wrapper__squeeze));
  m.impl("squeeze.dim",
  TORCH_FN(wrapper_dim_squeeze));
  m.impl("squeeze_",
  TORCH_FN(wrapper__squeeze_));
  m.impl("squeeze_.dim",
  TORCH_FN(wrapper_dim_squeeze_));
  m.impl("stack",
  TORCH_FN(wrapper__stack));
  m.impl("stack.out",
  TORCH_FN(wrapper_out_stack_out));
  m.impl("std",
  TORCH_FN(wrapper__std));
  m.impl("std.dim",
  TORCH_FN(wrapper_dim_std));
  m.impl("std.out",
  TORCH_FN(wrapper_out_std_out));
  m.impl("std.correction",
  TORCH_FN(wrapper_correction_std));
  m.impl("std.correction_out",
  TORCH_FN(wrapper_correction_out_std_out));
  m.impl("sub.Tensor",
  TORCH_FN(wrapper_Tensor_sub));
  m.impl("sub.out",
  TORCH_FN(wrapper_out_sub_out));
  m.impl("sub_.Tensor",
  TORCH_FN(wrapper_Tensor_sub_));
  m.impl("sum",
  TORCH_FN(wrapper__sum));
  m.impl("sum.dim_IntList",
  TORCH_FN(wrapper_dim_IntList_sum));
  m.impl("sum.IntList_out",
  TORCH_FN(wrapper_IntList_out_sum_out));
  m.impl("t",
  TORCH_FN(wrapper__t));
  m.impl("t_",
  TORCH_FN(wrapper__t_));
  m.impl("tanh",
  TORCH_FN(wrapper__tanh));
  m.impl("tanh.out",
  TORCH_FN(wrapper_out_tanh_out));
  m.impl("tanh_",
  TORCH_FN(wrapper__tanh_));
  m.impl("tanh_backward",
  TORCH_FN(wrapper__tanh_backward));
  m.impl("tanh_backward.grad_input",
  TORCH_FN(wrapper_grad_input_tanh_backward_out));
  m.impl("threshold",
  TORCH_FN(wrapper__threshold));
  m.impl("threshold.out",
  TORCH_FN(wrapper_out_threshold_out));
  m.impl("threshold_",
  TORCH_FN(wrapper__threshold_));
  m.impl("threshold_backward",
  TORCH_FN(wrapper__threshold_backward));
  m.impl("threshold_backward.grad_input",
  TORCH_FN(wrapper_grad_input_threshold_backward_out));
  m.impl("topk",
  TORCH_FN(wrapper__topk));
  m.impl("topk.values",
  TORCH_FN(wrapper_values_topk_out));
  m.impl("trace",
  TORCH_FN(wrapper__trace));
  m.impl("transpose.int",
  TORCH_FN(wrapper_int_transpose));
  m.impl("transpose_",
  TORCH_FN(wrapper__transpose_));
  m.impl("tril",
  TORCH_FN(wrapper__tril));
  m.impl("tril.out",
  TORCH_FN(wrapper_out_tril_out));
  m.impl("tril_",
  TORCH_FN(wrapper__tril_));
  m.impl("triu",
  TORCH_FN(wrapper__triu));
  m.impl("triu.out",
  TORCH_FN(wrapper_out_triu_out));
  m.impl("triu_",
  TORCH_FN(wrapper__triu_));
  m.impl("trunc",
  TORCH_FN(wrapper__trunc));
  m.impl("trunc.out",
  TORCH_FN(wrapper_out_trunc_out));
  m.impl("trunc_",
  TORCH_FN(wrapper__trunc_));
  m.impl("unsqueeze",
  TORCH_FN(wrapper__unsqueeze));
  m.impl("unsqueeze_",
  TORCH_FN(wrapper__unsqueeze_));
  m.impl("upsample_bilinear2d",
  TORCH_FN(wrapper__upsample_bilinear2d));
  m.impl("upsample_bilinear2d.out",
  TORCH_FN(wrapper_out_upsample_bilinear2d_out));
  m.impl("upsample_bilinear2d_backward",
  TORCH_FN(wrapper__upsample_bilinear2d_backward));
  m.impl("upsample_bilinear2d_backward.grad_input",
  TORCH_FN(wrapper_grad_input_upsample_bilinear2d_backward_out));
  m.impl("upsample_nearest2d",
  TORCH_FN(wrapper__upsample_nearest2d));
  m.impl("upsample_nearest2d.out",
  TORCH_FN(wrapper_out_upsample_nearest2d_out));
  m.impl("upsample_nearest2d_backward",
  TORCH_FN(wrapper__upsample_nearest2d_backward));
  m.impl("upsample_nearest2d_backward.grad_input",
  TORCH_FN(wrapper_grad_input_upsample_nearest2d_backward_out));
  m.impl("view",
  TORCH_FN(wrapper__view));
  m.impl("zero_",
  TORCH_FN(wrapper__zero_));
}

} // anonymous namespace

namespace lazy {



} // namespace lazy

} // namespace at
