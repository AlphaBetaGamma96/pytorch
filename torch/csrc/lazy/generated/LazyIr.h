#pragma once

// This file contains autogenerated LazyTensor IR nodes
#include <ATen/core/Formatting.h>
#include <c10/core/ScalarType.h>
#include <c10/util/Optional.h>
#include <torch/csrc/lazy/core/hash.h>
#include <torch/csrc/lazy/core/ir.h>
#include <torch/csrc/lazy/core/shape.h>
#include <vector>
#include "torch/csrc/lazy/ts_backend/ts_node.h"

namespace torch {
namespace lazy {
using at::operator<<;

// kNullValue is used to contribute a static hash value any time
// a node has an Optional<Value> input that is nullopt.  It is important
// to differentiate between HASH(nullopt, something) and HASH(something, nullopt),
// and using kNullValue in the hash function in the order of arguments
// serves this purpose.
static const torch::lazy::Value kNullValue = torch::lazy::Value();

class AdaptiveAvgPool2d : public TsNode {
 public:
  AdaptiveAvgPool2d(const torch::lazy::Value& self, const ::std::vector<int64_t>& output_size, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::_adaptive_avg_pool2d),
              {self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(output_size)),

        output_size(output_size)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", output_size=" << output_size;
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(2);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back("output_size", output_size);
    
    torch::lazy::TSOpVector _adaptive_avg_pool2d_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(_adaptive_avg_pool2d_out.size(), 1);

    return _adaptive_avg_pool2d_out;

  }

  ::std::vector<int64_t> output_size;
  

};

class AdaptiveAvgPool2dBackward : public TsNode {
 public:
  AdaptiveAvgPool2dBackward(const torch::lazy::Value& grad_output, const torch::lazy::Value& self, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::_adaptive_avg_pool2d_backward),
              {grad_output, self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(2);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector _adaptive_avg_pool2d_backward_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(_adaptive_avg_pool2d_backward_out.size(), 1);

    return _adaptive_avg_pool2d_backward_out;

  }

  
  

};

class LogSoftmax : public TsNode {
 public:
  LogSoftmax(const torch::lazy::Value& self, const int64_t& dim, const bool& half_to_float, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::_log_softmax),
              {self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(dim, half_to_float)),

        dim(dim),
        half_to_float(half_to_float)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", dim=" << dim;
    ss << ", half_to_float=" << half_to_float;
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(3);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back("dim", dim);
    arguments.emplace_back("half_to_float", half_to_float);
    
    torch::lazy::TSOpVector _log_softmax_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(_log_softmax_out.size(), 1);

    return _log_softmax_out;

  }

  int64_t dim;
  bool half_to_float;
  

};

class LogSoftmaxBackwardData : public TsNode {
 public:
  LogSoftmaxBackwardData(const torch::lazy::Value& grad_output, const torch::lazy::Value& output, const int64_t& dim, const at::ScalarType& input_dtype, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::_log_softmax_backward_data),
              {grad_output, output}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(dim, input_dtype)),

        dim(dim),
        input_dtype(input_dtype)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", dim=" << dim;
    ss << ", input_dtype=" << input_dtype;
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(4);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back("dim", dim);
    arguments.emplace_back("input_dtype", input_dtype);
    
    torch::lazy::TSOpVector _log_softmax_backward_data_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(_log_softmax_backward_data_out.size(), 1);

    return _log_softmax_backward_data_out;

  }

  int64_t dim;
  at::ScalarType input_dtype;
  

};

class Softmax : public TsNode {
 public:
  Softmax(const torch::lazy::Value& self, const int64_t& dim, const bool& half_to_float, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::_softmax),
              {self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(dim, half_to_float)),

        dim(dim),
        half_to_float(half_to_float)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", dim=" << dim;
    ss << ", half_to_float=" << half_to_float;
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(3);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back("dim", dim);
    arguments.emplace_back("half_to_float", half_to_float);
    
    torch::lazy::TSOpVector _softmax_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(_softmax_out.size(), 1);

    return _softmax_out;

  }

  int64_t dim;
  bool half_to_float;
  

};

class SoftmaxBackwardData : public TsNode {
 public:
  SoftmaxBackwardData(const torch::lazy::Value& grad_output, const torch::lazy::Value& output, const int64_t& dim, const at::ScalarType& input_dtype, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::_softmax_backward_data),
              {grad_output, output}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(dim, input_dtype)),

        dim(dim),
        input_dtype(input_dtype)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", dim=" << dim;
    ss << ", input_dtype=" << input_dtype;
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(4);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back("dim", dim);
    arguments.emplace_back("input_dtype", input_dtype);
    
    torch::lazy::TSOpVector _softmax_backward_data_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(_softmax_backward_data_out.size(), 1);

    return _softmax_backward_data_out;

  }

  int64_t dim;
  at::ScalarType input_dtype;
  

};

class Abs : public TsNode {
 public:
  Abs(const torch::lazy::Value& self, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::abs),
              {self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(1);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector abs_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(abs_out.size(), 1);

    return abs_out;

  }

  
  

};

class AddTensor : public TsNode {
 public:
  AddTensor(const torch::lazy::Value& self, const torch::lazy::Value& other, const torch::lazy::Value& alpha, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::add),
              {self, other, alpha}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(2);
    kwarguments.reserve(1);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    kwarguments.emplace_back("alpha", loctx->GetOutputOp(operand(i++)));
    torch::lazy::TSOpVector add_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(add_out.size(), 1);

    return add_out;

  }

  
  

};

class Addcdiv : public TsNode {
 public:
  Addcdiv(const torch::lazy::Value& self, const torch::lazy::Value& tensor1, const torch::lazy::Value& tensor2, const torch::lazy::Value& value, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::addcdiv),
              {self, tensor1, tensor2, value}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(3);
    kwarguments.reserve(1);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    kwarguments.emplace_back("value", loctx->GetOutputOp(operand(i++)));
    torch::lazy::TSOpVector addcdiv_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(addcdiv_out.size(), 1);

    return addcdiv_out;

  }

  
  

};

class Addcmul : public TsNode {
 public:
  Addcmul(const torch::lazy::Value& self, const torch::lazy::Value& tensor1, const torch::lazy::Value& tensor2, const torch::lazy::Value& value, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::addcmul),
              {self, tensor1, tensor2, value}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(3);
    kwarguments.reserve(1);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    kwarguments.emplace_back("value", loctx->GetOutputOp(operand(i++)));
    torch::lazy::TSOpVector addcmul_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(addcmul_out.size(), 1);

    return addcmul_out;

  }

  
  

};

class Addmm : public TsNode {
 public:
  Addmm(const torch::lazy::Value& self, const torch::lazy::Value& mat1, const torch::lazy::Value& mat2, const torch::lazy::Value& beta, const torch::lazy::Value& alpha, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::addmm),
              {self, mat1, mat2, beta, alpha}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(3);
    kwarguments.reserve(2);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    kwarguments.emplace_back("beta", loctx->GetOutputOp(operand(i++)));
    kwarguments.emplace_back("alpha", loctx->GetOutputOp(operand(i++)));
    torch::lazy::TSOpVector addmm_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(addmm_out.size(), 1);

    return addmm_out;

  }

  
  

};

class All : public TsNode {
 public:
  All(const torch::lazy::Value& self, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::all),
              {self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(1);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector all_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(all_out.size(), 1);

    return all_out;

  }

  
  

};

class Any : public TsNode {
 public:
  Any(const torch::lazy::Value& self, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::any),
              {self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(1);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector any_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(any_out.size(), 1);

    return any_out;

  }

  
  

};

class ArangeStartOut : public TsNode {
 public:
  ArangeStartOut(const torch::lazy::Value& start, const torch::lazy::Value& end, const torch::lazy::Value& step, const torch::lazy::Value& out, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::arange),
              {start, end, step, out}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(3);
    kwarguments.reserve(1);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    kwarguments.emplace_back("out", loctx->GetOutputOp(operand(i++)));
    torch::lazy::TSOpVector arange_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(arange_out.size(), 1);

    return arange_out;

  }

  
  

};

class AvgPool2d : public TsNode {
 public:
  AvgPool2d(const torch::lazy::Value& self, const ::std::vector<int64_t>& kernel_size, const ::std::vector<int64_t>& stride, const ::std::vector<int64_t>& padding, const bool& ceil_mode, const bool& count_include_pad, const c10::optional<int64_t>& divisor_override, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::avg_pool2d),
              {self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override)),

        kernel_size(kernel_size),
        stride(stride),
        padding(padding),
        ceil_mode(ceil_mode),
        count_include_pad(count_include_pad),
        divisor_override(divisor_override)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", kernel_size=" << kernel_size;
    ss << ", stride=" << stride;
    ss << ", padding=" << padding;
    ss << ", ceil_mode=" << ceil_mode;
    ss << ", count_include_pad=" << count_include_pad;
    if (divisor_override.has_value()) {
    ss << ", divisor_override=" << divisor_override.value();
} else {
    ss << ", divisor_override=null";
}
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(7);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back("kernel_size", kernel_size);
    arguments.emplace_back("stride", stride);
    arguments.emplace_back("padding", padding);
    arguments.emplace_back("ceil_mode", ceil_mode);
    arguments.emplace_back("count_include_pad", count_include_pad);
    arguments.emplace_back("divisor_override", divisor_override);
    
    torch::lazy::TSOpVector avg_pool2d_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(avg_pool2d_out.size(), 1);

    return avg_pool2d_out;

  }

  ::std::vector<int64_t> kernel_size;
  ::std::vector<int64_t> stride;
  ::std::vector<int64_t> padding;
  bool ceil_mode;
  bool count_include_pad;
  c10::optional<int64_t> divisor_override;
  

};

class AvgPool2dBackward : public TsNode {
 public:
  AvgPool2dBackward(const torch::lazy::Value& grad_output, const torch::lazy::Value& self, const ::std::vector<int64_t>& kernel_size, const ::std::vector<int64_t>& stride, const ::std::vector<int64_t>& padding, const bool& ceil_mode, const bool& count_include_pad, const c10::optional<int64_t>& divisor_override, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::avg_pool2d_backward),
              {grad_output, self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override)),

        kernel_size(kernel_size),
        stride(stride),
        padding(padding),
        ceil_mode(ceil_mode),
        count_include_pad(count_include_pad),
        divisor_override(divisor_override)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", kernel_size=" << kernel_size;
    ss << ", stride=" << stride;
    ss << ", padding=" << padding;
    ss << ", ceil_mode=" << ceil_mode;
    ss << ", count_include_pad=" << count_include_pad;
    if (divisor_override.has_value()) {
    ss << ", divisor_override=" << divisor_override.value();
} else {
    ss << ", divisor_override=null";
}
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(8);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back("kernel_size", kernel_size);
    arguments.emplace_back("stride", stride);
    arguments.emplace_back("padding", padding);
    arguments.emplace_back("ceil_mode", ceil_mode);
    arguments.emplace_back("count_include_pad", count_include_pad);
    arguments.emplace_back("divisor_override", divisor_override);
    
    torch::lazy::TSOpVector avg_pool2d_backward_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(avg_pool2d_backward_out.size(), 1);

    return avg_pool2d_backward_out;

  }

  ::std::vector<int64_t> kernel_size;
  ::std::vector<int64_t> stride;
  ::std::vector<int64_t> padding;
  bool ceil_mode;
  bool count_include_pad;
  c10::optional<int64_t> divisor_override;
  

};

class Baddbmm : public TsNode {
 public:
  Baddbmm(const torch::lazy::Value& self, const torch::lazy::Value& batch1, const torch::lazy::Value& batch2, const torch::lazy::Value& beta, const torch::lazy::Value& alpha, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::baddbmm),
              {self, batch1, batch2, beta, alpha}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(3);
    kwarguments.reserve(2);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    kwarguments.emplace_back("beta", loctx->GetOutputOp(operand(i++)));
    kwarguments.emplace_back("alpha", loctx->GetOutputOp(operand(i++)));
    torch::lazy::TSOpVector baddbmm_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(baddbmm_out.size(), 1);

    return baddbmm_out;

  }

  
  

};

class Bernoulli : public TsNode {
 public:
  Bernoulli(const torch::lazy::Value& self, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::bernoulli),
              {self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(1);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector bernoulli_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(bernoulli_out.size(), 1);

    return bernoulli_out;

  }

  
  

};

class BernoulliFloat : public TsNode {
 public:
  BernoulliFloat(const torch::lazy::Value& self, const double& p, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::bernoulli_),
              {self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(p)),

        p(p)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", p=" << p;
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(2);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back("p", p);
    
    torch::lazy::TSOpVector bernoulli__out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(bernoulli__out.size(), 1);

    return bernoulli__out;

  }

  double p;
  

};

class BinaryCrossEntropy : public TsNode {
 public:
  BinaryCrossEntropy(const torch::lazy::Value& self, const torch::lazy::Value& target, const c10::optional<torch::lazy::Value>& weight, const int64_t& reduction, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::binary_cross_entropy),
              {self, target, weight.value_or(kNullValue)}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(reduction)),

        reduction(reduction)

  {
    has_weight = !!weight;
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", reduction=" << reduction;
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(4);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(has_weight ? loctx->GetOutputOp(operand(i++)) : nullptr);
    arguments.emplace_back("reduction", reduction);
    
    torch::lazy::TSOpVector binary_cross_entropy_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(binary_cross_entropy_out.size(), 1);

    return binary_cross_entropy_out;

  }

  int64_t reduction;
  bool has_weight: 1;

};

class BinaryCrossEntropyBackward : public TsNode {
 public:
  BinaryCrossEntropyBackward(const torch::lazy::Value& grad_output, const torch::lazy::Value& self, const torch::lazy::Value& target, const c10::optional<torch::lazy::Value>& weight, const int64_t& reduction, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::binary_cross_entropy_backward),
              {grad_output, self, target, weight.value_or(kNullValue)}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(reduction)),

        reduction(reduction)

  {
    has_weight = !!weight;
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", reduction=" << reduction;
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(5);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(has_weight ? loctx->GetOutputOp(operand(i++)) : nullptr);
    arguments.emplace_back("reduction", reduction);
    
    torch::lazy::TSOpVector binary_cross_entropy_backward_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(binary_cross_entropy_backward_out.size(), 1);

    return binary_cross_entropy_backward_out;

  }

  int64_t reduction;
  bool has_weight: 1;

};

class BitwiseAndTensor : public TsNode {
 public:
  BitwiseAndTensor(const torch::lazy::Value& self, const torch::lazy::Value& other, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::bitwise_and),
              {self, other}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(2);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector bitwise_and_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(bitwise_and_out.size(), 1);

    return bitwise_and_out;

  }

  
  

};

class BitwiseOrTensor : public TsNode {
 public:
  BitwiseOrTensor(const torch::lazy::Value& self, const torch::lazy::Value& other, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::bitwise_or),
              {self, other}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(2);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector bitwise_or_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(bitwise_or_out.size(), 1);

    return bitwise_or_out;

  }

  
  

};

class Bmm : public TsNode {
 public:
  Bmm(const torch::lazy::Value& self, const torch::lazy::Value& mat2, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::bmm),
              {self, mat2}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(2);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector bmm_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(bmm_out.size(), 1);

    return bmm_out;

  }

  
  

};

class Cat : public TsNode {
 public:
  Cat(const torch::lazy::Value& tensors, const int64_t& dim, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::cat),
              {tensors}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(dim)),

        dim(dim)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", dim=" << dim;
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(2);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back("dim", dim);
    
    torch::lazy::TSOpVector cat_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(cat_out.size(), 1);

    return cat_out;

  }

  int64_t dim;
  

};

class Clamp : public TsNode {
 public:
  Clamp(const torch::lazy::Value& self, const c10::optional<torch::lazy::Value>& min, const c10::optional<torch::lazy::Value>& max, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::clamp),
              {self, min.value_or(kNullValue), max.value_or(kNullValue)}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    has_min = !!min;
    has_max = !!max;
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(3);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(has_min ? loctx->GetOutputOp(operand(i++)) : nullptr);
    arguments.emplace_back(has_max ? loctx->GetOutputOp(operand(i++)) : nullptr);
    
    torch::lazy::TSOpVector clamp_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(clamp_out.size(), 1);

    return clamp_out;

  }

  
  bool has_min: 1;
  bool has_max: 1;

};

class ClampMin : public TsNode {
 public:
  ClampMin(const torch::lazy::Value& self, const torch::lazy::Value& min, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::clamp_min),
              {self, min}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(2);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector clamp_min_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(clamp_min_out.size(), 1);

    return clamp_min_out;

  }

  
  

};

class ConstantPadNd : public TsNode {
 public:
  ConstantPadNd(const torch::lazy::Value& self, const ::std::vector<int64_t>& pad, const torch::lazy::Value& value, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::constant_pad_nd),
              {self, value}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(pad)),

        pad(pad)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", pad=" << pad;
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(3);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back("pad", pad);
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector constant_pad_nd_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(constant_pad_nd_out.size(), 1);

    return constant_pad_nd_out;

  }

  ::std::vector<int64_t> pad;
  

};

class Convolution : public TsNode {
 public:
  Convolution(const torch::lazy::Value& input, const torch::lazy::Value& weight, const c10::optional<torch::lazy::Value>& bias, const ::std::vector<int64_t>& stride, const ::std::vector<int64_t>& padding, const ::std::vector<int64_t>& dilation, const bool& transposed, const ::std::vector<int64_t>& output_padding, const int64_t& groups, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::convolution),
              {input, weight, bias.value_or(kNullValue)}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(stride, padding, dilation, transposed, output_padding, groups)),

        stride(stride),
        padding(padding),
        dilation(dilation),
        transposed(transposed),
        output_padding(output_padding),
        groups(groups)

  {
    has_bias = !!bias;
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", stride=" << stride;
    ss << ", padding=" << padding;
    ss << ", dilation=" << dilation;
    ss << ", transposed=" << transposed;
    ss << ", output_padding=" << output_padding;
    ss << ", groups=" << groups;
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(9);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(has_bias ? loctx->GetOutputOp(operand(i++)) : nullptr);
    arguments.emplace_back("stride", stride);
    arguments.emplace_back("padding", padding);
    arguments.emplace_back("dilation", dilation);
    arguments.emplace_back("transposed", transposed);
    arguments.emplace_back("output_padding", output_padding);
    arguments.emplace_back("groups", groups);
    
    torch::lazy::TSOpVector convolution_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(convolution_out.size(), 1);

    return convolution_out;

  }

  ::std::vector<int64_t> stride;
  ::std::vector<int64_t> padding;
  ::std::vector<int64_t> dilation;
  bool transposed;
  ::std::vector<int64_t> output_padding;
  int64_t groups;
  bool has_bias: 1;

};

class ConvolutionBackward : public TsNode {
 public:
  ConvolutionBackward(const torch::lazy::Value& grad_output, const torch::lazy::Value& input, const torch::lazy::Value& weight, const c10::optional<::std::vector<int64_t>>& bias_sizes, const ::std::vector<int64_t>& stride, const ::std::vector<int64_t>& padding, const ::std::vector<int64_t>& dilation, const bool& transposed, const ::std::vector<int64_t>& output_padding, const int64_t& groups, const ::std::vector<bool>& output_mask, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::convolution_backward),
              {grad_output, input, weight}, std::move(shapes),
              /* num_outputs */ 3,
              torch::lazy::MHash(bias_sizes, stride, padding, dilation, transposed, output_padding, groups, output_mask)),

        bias_sizes(bias_sizes),
        stride(stride),
        padding(padding),
        dilation(dilation),
        transposed(transposed),
        output_padding(output_padding),
        groups(groups),
        output_mask(output_mask)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    if (bias_sizes.has_value()) {
    ss << ", bias_sizes=" << bias_sizes.value();
} else {
    ss << ", bias_sizes=null";
}
    ss << ", stride=" << stride;
    ss << ", padding=" << padding;
    ss << ", dilation=" << dilation;
    ss << ", transposed=" << transposed;
    ss << ", output_padding=" << output_padding;
    ss << ", groups=" << groups;
    ss << ", output_mask=" << output_mask;
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(11);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back("bias_sizes", bias_sizes);
    arguments.emplace_back("stride", stride);
    arguments.emplace_back("padding", padding);
    arguments.emplace_back("dilation", dilation);
    arguments.emplace_back("transposed", transposed);
    arguments.emplace_back("output_padding", output_padding);
    arguments.emplace_back("groups", groups);
    arguments.emplace_back("output_mask", output_mask);
    
    torch::lazy::TSOpVector convolution_backward_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(convolution_backward_out.size(), 3);

    return convolution_backward_out;

  }

  c10::optional<::std::vector<int64_t>> bias_sizes;
  ::std::vector<int64_t> stride;
  ::std::vector<int64_t> padding;
  ::std::vector<int64_t> dilation;
  bool transposed;
  ::std::vector<int64_t> output_padding;
  int64_t groups;
  ::std::vector<bool> output_mask;
  

};

class Cos : public TsNode {
 public:
  Cos(const torch::lazy::Value& self, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::cos),
              {self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(1);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector cos_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(cos_out.size(), 1);

    return cos_out;

  }

  
  

};

class Cumsum : public TsNode {
 public:
  Cumsum(const torch::lazy::Value& self, const int64_t& dim, const c10::optional<at::ScalarType>& dtype, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::cumsum),
              {self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(dim, dtype)),

        dim(dim),
        dtype(dtype)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", dim=" << dim;
    if (dtype.has_value()) {
    ss << ", dtype=" << dtype.value();
} else {
    ss << ", dtype=null";
}
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(2);
    kwarguments.reserve(1);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back("dim", dim);
    kwarguments.emplace_back("dtype", dtype);
    torch::lazy::TSOpVector cumsum_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(cumsum_out.size(), 1);

    return cumsum_out;

  }

  int64_t dim;
  c10::optional<at::ScalarType> dtype;
  

};

class DivTensor : public TsNode {
 public:
  DivTensor(const torch::lazy::Value& self, const torch::lazy::Value& other, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::div),
              {self, other}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(2);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector div_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(div_out.size(), 1);

    return div_out;

  }

  
  

};

class DivTensorMode : public TsNode {
 public:
  DivTensorMode(const torch::lazy::Value& self, const torch::lazy::Value& other, const c10::optional<c10::string_view>& rounding_mode, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::div),
              {self, other}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(rounding_mode)),

        rounding_mode(rounding_mode)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    if (rounding_mode.has_value()) {
    ss << ", rounding_mode=" << rounding_mode.value();
} else {
    ss << ", rounding_mode=null";
}
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(2);
    kwarguments.reserve(1);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    kwarguments.emplace_back("rounding_mode", rounding_mode);
    torch::lazy::TSOpVector div_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(div_out.size(), 1);

    return div_out;

  }

  c10::optional<c10::string_view> rounding_mode;
  

};

class Elu : public TsNode {
 public:
  Elu(const torch::lazy::Value& self, const torch::lazy::Value& alpha, const torch::lazy::Value& scale, const torch::lazy::Value& input_scale, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::elu),
              {self, alpha, scale, input_scale}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(4);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector elu_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(elu_out.size(), 1);

    return elu_out;

  }

  
  

};

class EluBackward : public TsNode {
 public:
  EluBackward(const torch::lazy::Value& grad_output, const torch::lazy::Value& alpha, const torch::lazy::Value& scale, const torch::lazy::Value& input_scale, const bool& is_result, const torch::lazy::Value& self_or_result, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::elu_backward),
              {grad_output, alpha, scale, input_scale, self_or_result}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(is_result)),

        is_result(is_result)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", is_result=" << is_result;
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(6);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back("is_result", is_result);
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector elu_backward_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(elu_backward_out.size(), 1);

    return elu_backward_out;

  }

  bool is_result;
  

};

class Embedding : public TsNode {
 public:
  Embedding(const torch::lazy::Value& weight, const torch::lazy::Value& indices, const int64_t& padding_idx, const bool& scale_grad_by_freq, const bool& sparse, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::embedding),
              {weight, indices}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(padding_idx, scale_grad_by_freq, sparse)),

        padding_idx(padding_idx),
        scale_grad_by_freq(scale_grad_by_freq),
        sparse(sparse)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", padding_idx=" << padding_idx;
    ss << ", scale_grad_by_freq=" << scale_grad_by_freq;
    ss << ", sparse=" << sparse;
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(5);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back("padding_idx", padding_idx);
    arguments.emplace_back("scale_grad_by_freq", scale_grad_by_freq);
    arguments.emplace_back("sparse", sparse);
    
    torch::lazy::TSOpVector embedding_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(embedding_out.size(), 1);

    return embedding_out;

  }

  int64_t padding_idx;
  bool scale_grad_by_freq;
  bool sparse;
  

};

class EmbeddingDenseBackward : public TsNode {
 public:
  EmbeddingDenseBackward(const torch::lazy::Value& grad_output, const torch::lazy::Value& indices, const int64_t& num_weights, const int64_t& padding_idx, const bool& scale_grad_by_freq, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::embedding_dense_backward),
              {grad_output, indices}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(num_weights, padding_idx, scale_grad_by_freq)),

        num_weights(num_weights),
        padding_idx(padding_idx),
        scale_grad_by_freq(scale_grad_by_freq)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", num_weights=" << num_weights;
    ss << ", padding_idx=" << padding_idx;
    ss << ", scale_grad_by_freq=" << scale_grad_by_freq;
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(5);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back("num_weights", num_weights);
    arguments.emplace_back("padding_idx", padding_idx);
    arguments.emplace_back("scale_grad_by_freq", scale_grad_by_freq);
    
    torch::lazy::TSOpVector embedding_dense_backward_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(embedding_dense_backward_out.size(), 1);

    return embedding_dense_backward_out;

  }

  int64_t num_weights;
  int64_t padding_idx;
  bool scale_grad_by_freq;
  

};

class EqScalar : public TsNode {
 public:
  EqScalar(const torch::lazy::Value& self, const torch::lazy::Value& other, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::eq),
              {self, other}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(2);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector eq_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(eq_out.size(), 1);

    return eq_out;

  }

  
  

};

class EqTensor : public TsNode {
 public:
  EqTensor(const torch::lazy::Value& self, const torch::lazy::Value& other, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::eq),
              {self, other}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(2);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector eq_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(eq_out.size(), 1);

    return eq_out;

  }

  
  

};

class Exp : public TsNode {
 public:
  Exp(const torch::lazy::Value& self, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::exp),
              {self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(1);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector exp_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(exp_out.size(), 1);

    return exp_out;

  }

  
  

};

class Flip : public TsNode {
 public:
  Flip(const torch::lazy::Value& self, const ::std::vector<int64_t>& dims, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::flip),
              {self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(dims)),

        dims(dims)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", dims=" << dims;
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(2);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back("dims", dims);
    
    torch::lazy::TSOpVector flip_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(flip_out.size(), 1);

    return flip_out;

  }

  ::std::vector<int64_t> dims;
  

};

class Floor : public TsNode {
 public:
  Floor(const torch::lazy::Value& self, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::floor),
              {self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(1);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector floor_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(floor_out.size(), 1);

    return floor_out;

  }

  
  

};

class Frac : public TsNode {
 public:
  Frac(const torch::lazy::Value& self, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::frac),
              {self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(1);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector frac_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(frac_out.size(), 1);

    return frac_out;

  }

  
  

};

class Gather : public TsNode {
 public:
  Gather(const torch::lazy::Value& self, const int64_t& dim, const torch::lazy::Value& index, const bool& sparse_grad, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::gather),
              {self, index}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(dim, sparse_grad)),

        dim(dim),
        sparse_grad(sparse_grad)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", dim=" << dim;
    ss << ", sparse_grad=" << sparse_grad;
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(3);
    kwarguments.reserve(1);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back("dim", dim);
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    kwarguments.emplace_back("sparse_grad", sparse_grad);
    torch::lazy::TSOpVector gather_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(gather_out.size(), 1);

    return gather_out;

  }

  int64_t dim;
  bool sparse_grad;
  

};

class GeScalar : public TsNode {
 public:
  GeScalar(const torch::lazy::Value& self, const torch::lazy::Value& other, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::ge),
              {self, other}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(2);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector ge_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(ge_out.size(), 1);

    return ge_out;

  }

  
  

};

class GeTensor : public TsNode {
 public:
  GeTensor(const torch::lazy::Value& self, const torch::lazy::Value& other, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::ge),
              {self, other}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(2);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector ge_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(ge_out.size(), 1);

    return ge_out;

  }

  
  

};

class Gelu : public TsNode {
 public:
  Gelu(const torch::lazy::Value& self, const c10::string_view& approximate, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::gelu),
              {self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(approximate)),

        approximate(approximate)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", approximate=" << approximate;
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(1);
    kwarguments.reserve(1);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    kwarguments.emplace_back("approximate", approximate);
    torch::lazy::TSOpVector gelu_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(gelu_out.size(), 1);

    return gelu_out;

  }

  std::string approximate;
  

};

class GeluBackward : public TsNode {
 public:
  GeluBackward(const torch::lazy::Value& grad_output, const torch::lazy::Value& self, const c10::string_view& approximate, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::gelu_backward),
              {grad_output, self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(approximate)),

        approximate(approximate)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", approximate=" << approximate;
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(2);
    kwarguments.reserve(1);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    kwarguments.emplace_back("approximate", approximate);
    torch::lazy::TSOpVector gelu_backward_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(gelu_backward_out.size(), 1);

    return gelu_backward_out;

  }

  std::string approximate;
  

};

class Glu : public TsNode {
 public:
  Glu(const torch::lazy::Value& self, const int64_t& dim, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::glu),
              {self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(dim)),

        dim(dim)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", dim=" << dim;
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(2);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back("dim", dim);
    
    torch::lazy::TSOpVector glu_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(glu_out.size(), 1);

    return glu_out;

  }

  int64_t dim;
  

};

class GluBackward : public TsNode {
 public:
  GluBackward(const torch::lazy::Value& grad_output, const torch::lazy::Value& self, const int64_t& dim, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::glu_backward),
              {grad_output, self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(dim)),

        dim(dim)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", dim=" << dim;
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(3);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back("dim", dim);
    
    torch::lazy::TSOpVector glu_backward_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(glu_backward_out.size(), 1);

    return glu_backward_out;

  }

  int64_t dim;
  

};

class GridSampler2d : public TsNode {
 public:
  GridSampler2d(const torch::lazy::Value& input, const torch::lazy::Value& grid, const int64_t& interpolation_mode, const int64_t& padding_mode, const bool& align_corners, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::grid_sampler_2d),
              {input, grid}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(interpolation_mode, padding_mode, align_corners)),

        interpolation_mode(interpolation_mode),
        padding_mode(padding_mode),
        align_corners(align_corners)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", interpolation_mode=" << interpolation_mode;
    ss << ", padding_mode=" << padding_mode;
    ss << ", align_corners=" << align_corners;
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(5);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back("interpolation_mode", interpolation_mode);
    arguments.emplace_back("padding_mode", padding_mode);
    arguments.emplace_back("align_corners", align_corners);
    
    torch::lazy::TSOpVector grid_sampler_2d_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(grid_sampler_2d_out.size(), 1);

    return grid_sampler_2d_out;

  }

  int64_t interpolation_mode;
  int64_t padding_mode;
  bool align_corners;
  

};

class GridSampler2dBackward : public TsNode {
 public:
  GridSampler2dBackward(const torch::lazy::Value& grad_output, const torch::lazy::Value& input, const torch::lazy::Value& grid, const int64_t& interpolation_mode, const int64_t& padding_mode, const bool& align_corners, const ::std::vector<bool>& output_mask, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::grid_sampler_2d_backward),
              {grad_output, input, grid}, std::move(shapes),
              /* num_outputs */ 2,
              torch::lazy::MHash(interpolation_mode, padding_mode, align_corners, output_mask)),

        interpolation_mode(interpolation_mode),
        padding_mode(padding_mode),
        align_corners(align_corners),
        output_mask(output_mask)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", interpolation_mode=" << interpolation_mode;
    ss << ", padding_mode=" << padding_mode;
    ss << ", align_corners=" << align_corners;
    ss << ", output_mask=" << output_mask;
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(7);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back("interpolation_mode", interpolation_mode);
    arguments.emplace_back("padding_mode", padding_mode);
    arguments.emplace_back("align_corners", align_corners);
    arguments.emplace_back("output_mask", output_mask);
    
    torch::lazy::TSOpVector grid_sampler_2d_backward_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(grid_sampler_2d_backward_out.size(), 2);

    return grid_sampler_2d_backward_out;

  }

  int64_t interpolation_mode;
  int64_t padding_mode;
  bool align_corners;
  ::std::vector<bool> output_mask;
  

};

class GtScalar : public TsNode {
 public:
  GtScalar(const torch::lazy::Value& self, const torch::lazy::Value& other, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::gt),
              {self, other}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(2);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector gt_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(gt_out.size(), 1);

    return gt_out;

  }

  
  

};

class GtTensor : public TsNode {
 public:
  GtTensor(const torch::lazy::Value& self, const torch::lazy::Value& other, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::gt),
              {self, other}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(2);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector gt_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(gt_out.size(), 1);

    return gt_out;

  }

  
  

};

class Hardsigmoid : public TsNode {
 public:
  Hardsigmoid(const torch::lazy::Value& self, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::hardsigmoid),
              {self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(1);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector hardsigmoid_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(hardsigmoid_out.size(), 1);

    return hardsigmoid_out;

  }

  
  

};

class IndexSelect : public TsNode {
 public:
  IndexSelect(const torch::lazy::Value& self, const int64_t& dim, const torch::lazy::Value& index, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::index_select),
              {self, index}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(dim)),

        dim(dim)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", dim=" << dim;
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(3);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back("dim", dim);
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector index_select_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(index_select_out.size(), 1);

    return index_select_out;

  }

  int64_t dim;
  

};

class KlDivBackward : public TsNode {
 public:
  KlDivBackward(const torch::lazy::Value& grad_output, const torch::lazy::Value& self, const torch::lazy::Value& target, const int64_t& reduction, const bool& log_target, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::kl_div_backward),
              {grad_output, self, target}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(reduction, log_target)),

        reduction(reduction),
        log_target(log_target)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", reduction=" << reduction;
    ss << ", log_target=" << log_target;
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(4);
    kwarguments.reserve(1);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back("reduction", reduction);
    kwarguments.emplace_back("log_target", log_target);
    torch::lazy::TSOpVector kl_div_backward_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(kl_div_backward_out.size(), 1);

    return kl_div_backward_out;

  }

  int64_t reduction;
  bool log_target;
  

};

class L1LossBackward : public TsNode {
 public:
  L1LossBackward(const torch::lazy::Value& grad_output, const torch::lazy::Value& self, const torch::lazy::Value& target, const int64_t& reduction, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::l1_loss_backward),
              {grad_output, self, target}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(reduction)),

        reduction(reduction)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", reduction=" << reduction;
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(4);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back("reduction", reduction);
    
    torch::lazy::TSOpVector l1_loss_backward_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(l1_loss_backward_out.size(), 1);

    return l1_loss_backward_out;

  }

  int64_t reduction;
  

};

class LeScalar : public TsNode {
 public:
  LeScalar(const torch::lazy::Value& self, const torch::lazy::Value& other, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::le),
              {self, other}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(2);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector le_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(le_out.size(), 1);

    return le_out;

  }

  
  

};

class LeTensor : public TsNode {
 public:
  LeTensor(const torch::lazy::Value& self, const torch::lazy::Value& other, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::le),
              {self, other}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(2);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector le_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(le_out.size(), 1);

    return le_out;

  }

  
  

};

class LeakyRelu : public TsNode {
 public:
  LeakyRelu(const torch::lazy::Value& self, const torch::lazy::Value& negative_slope, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::leaky_relu),
              {self, negative_slope}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(2);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector leaky_relu_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(leaky_relu_out.size(), 1);

    return leaky_relu_out;

  }

  
  

};

class LeakyReluBackward : public TsNode {
 public:
  LeakyReluBackward(const torch::lazy::Value& grad_output, const torch::lazy::Value& self, const torch::lazy::Value& negative_slope, const bool& self_is_result, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::leaky_relu_backward),
              {grad_output, self, negative_slope}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(self_is_result)),

        self_is_result(self_is_result)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", self_is_result=" << self_is_result;
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(4);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back("self_is_result", self_is_result);
    
    torch::lazy::TSOpVector leaky_relu_backward_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(leaky_relu_backward_out.size(), 1);

    return leaky_relu_backward_out;

  }

  bool self_is_result;
  

};

class Log : public TsNode {
 public:
  Log(const torch::lazy::Value& self, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::log),
              {self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(1);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector log_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(log_out.size(), 1);

    return log_out;

  }

  
  

};

class Log2 : public TsNode {
 public:
  Log2(const torch::lazy::Value& self, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::log2),
              {self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(1);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector log2_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(log2_out.size(), 1);

    return log2_out;

  }

  
  

};

class LogSigmoidBackward : public TsNode {
 public:
  LogSigmoidBackward(const torch::lazy::Value& grad_output, const torch::lazy::Value& self, const torch::lazy::Value& buffer, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::log_sigmoid_backward),
              {grad_output, self, buffer}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(3);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector log_sigmoid_backward_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(log_sigmoid_backward_out.size(), 1);

    return log_sigmoid_backward_out;

  }

  
  

};

class LogSigmoidForward : public TsNode {
 public:
  LogSigmoidForward(const torch::lazy::Value& self, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::log_sigmoid_forward),
              {self}, std::move(shapes),
              /* num_outputs */ 2,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(1);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector log_sigmoid_forward_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(log_sigmoid_forward_out.size(), 2);

    return log_sigmoid_forward_out;

  }

  
  

};

class Logdet : public TsNode {
 public:
  Logdet(const torch::lazy::Value& self, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::logdet),
              {self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(1);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector logdet_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(logdet_out.size(), 1);

    return logdet_out;

  }

  
  

};

class LtScalar : public TsNode {
 public:
  LtScalar(const torch::lazy::Value& self, const torch::lazy::Value& other, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::lt),
              {self, other}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(2);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector lt_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(lt_out.size(), 1);

    return lt_out;

  }

  
  

};

class LtTensor : public TsNode {
 public:
  LtTensor(const torch::lazy::Value& self, const torch::lazy::Value& other, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::lt),
              {self, other}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(2);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector lt_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(lt_out.size(), 1);

    return lt_out;

  }

  
  

};

class MaskedFillScalar : public TsNode {
 public:
  MaskedFillScalar(const torch::lazy::Value& self, const torch::lazy::Value& mask, const torch::lazy::Value& value, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::masked_fill_),
              {self, mask, value}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(3);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector masked_fill__out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(masked_fill__out.size(), 1);

    return masked_fill__out;

  }

  
  

};

class MaskedFillTensor : public TsNode {
 public:
  MaskedFillTensor(const torch::lazy::Value& self, const torch::lazy::Value& mask, const torch::lazy::Value& value, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::masked_fill_),
              {self, mask, value}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(3);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector masked_fill__out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(masked_fill__out.size(), 1);

    return masked_fill__out;

  }

  
  

};

class MaxDim : public TsNode {
 public:
  MaxDim(const torch::lazy::Value& self, const int64_t& dim, const bool& keepdim, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::max),
              {self}, std::move(shapes),
              /* num_outputs */ 2,
              torch::lazy::MHash(dim, keepdim)),

        dim(dim),
        keepdim(keepdim)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", dim=" << dim;
    ss << ", keepdim=" << keepdim;
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(3);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back("dim", dim);
    arguments.emplace_back("keepdim", keepdim);
    
    torch::lazy::TSOpVector max_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(max_out.size(), 2);

    return max_out;

  }

  int64_t dim;
  bool keepdim;
  

};

class Max : public TsNode {
 public:
  Max(const torch::lazy::Value& self, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::max),
              {self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(1);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector max_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(max_out.size(), 1);

    return max_out;

  }

  
  

};

class MaxPool2dWithIndices : public TsNode {
 public:
  MaxPool2dWithIndices(const torch::lazy::Value& self, const ::std::vector<int64_t>& kernel_size, const ::std::vector<int64_t>& stride, const ::std::vector<int64_t>& padding, const ::std::vector<int64_t>& dilation, const bool& ceil_mode, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::max_pool2d_with_indices),
              {self}, std::move(shapes),
              /* num_outputs */ 2,
              torch::lazy::MHash(kernel_size, stride, padding, dilation, ceil_mode)),

        kernel_size(kernel_size),
        stride(stride),
        padding(padding),
        dilation(dilation),
        ceil_mode(ceil_mode)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", kernel_size=" << kernel_size;
    ss << ", stride=" << stride;
    ss << ", padding=" << padding;
    ss << ", dilation=" << dilation;
    ss << ", ceil_mode=" << ceil_mode;
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(6);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back("kernel_size", kernel_size);
    arguments.emplace_back("stride", stride);
    arguments.emplace_back("padding", padding);
    arguments.emplace_back("dilation", dilation);
    arguments.emplace_back("ceil_mode", ceil_mode);
    
    torch::lazy::TSOpVector max_pool2d_with_indices_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(max_pool2d_with_indices_out.size(), 2);

    return max_pool2d_with_indices_out;

  }

  ::std::vector<int64_t> kernel_size;
  ::std::vector<int64_t> stride;
  ::std::vector<int64_t> padding;
  ::std::vector<int64_t> dilation;
  bool ceil_mode;
  

};

class MaxPool2dWithIndicesBackward : public TsNode {
 public:
  MaxPool2dWithIndicesBackward(const torch::lazy::Value& grad_output, const torch::lazy::Value& self, const ::std::vector<int64_t>& kernel_size, const ::std::vector<int64_t>& stride, const ::std::vector<int64_t>& padding, const ::std::vector<int64_t>& dilation, const bool& ceil_mode, const torch::lazy::Value& indices, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::max_pool2d_with_indices_backward),
              {grad_output, self, indices}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(kernel_size, stride, padding, dilation, ceil_mode)),

        kernel_size(kernel_size),
        stride(stride),
        padding(padding),
        dilation(dilation),
        ceil_mode(ceil_mode)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", kernel_size=" << kernel_size;
    ss << ", stride=" << stride;
    ss << ", padding=" << padding;
    ss << ", dilation=" << dilation;
    ss << ", ceil_mode=" << ceil_mode;
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(8);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back("kernel_size", kernel_size);
    arguments.emplace_back("stride", stride);
    arguments.emplace_back("padding", padding);
    arguments.emplace_back("dilation", dilation);
    arguments.emplace_back("ceil_mode", ceil_mode);
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector max_pool2d_with_indices_backward_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(max_pool2d_with_indices_backward_out.size(), 1);

    return max_pool2d_with_indices_backward_out;

  }

  ::std::vector<int64_t> kernel_size;
  ::std::vector<int64_t> stride;
  ::std::vector<int64_t> padding;
  ::std::vector<int64_t> dilation;
  bool ceil_mode;
  

};

class Maximum : public TsNode {
 public:
  Maximum(const torch::lazy::Value& self, const torch::lazy::Value& other, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::maximum),
              {self, other}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(2);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector maximum_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(maximum_out.size(), 1);

    return maximum_out;

  }

  
  

};

class Mean : public TsNode {
 public:
  Mean(const torch::lazy::Value& self, const c10::optional<at::ScalarType>& dtype, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::mean),
              {self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(dtype)),

        dtype(dtype)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    if (dtype.has_value()) {
    ss << ", dtype=" << dtype.value();
} else {
    ss << ", dtype=null";
}
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(1);
    kwarguments.reserve(1);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    kwarguments.emplace_back("dtype", dtype);
    torch::lazy::TSOpVector mean_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(mean_out.size(), 1);

    return mean_out;

  }

  c10::optional<at::ScalarType> dtype;
  

};

class MeanDim : public TsNode {
 public:
  MeanDim(const torch::lazy::Value& self, const ::std::vector<int64_t>& dim, const bool& keepdim, const c10::optional<at::ScalarType>& dtype, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::mean),
              {self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(dim, keepdim, dtype)),

        dim(dim),
        keepdim(keepdim),
        dtype(dtype)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", dim=" << dim;
    ss << ", keepdim=" << keepdim;
    if (dtype.has_value()) {
    ss << ", dtype=" << dtype.value();
} else {
    ss << ", dtype=null";
}
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(3);
    kwarguments.reserve(1);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back("dim", dim);
    arguments.emplace_back("keepdim", keepdim);
    kwarguments.emplace_back("dtype", dtype);
    torch::lazy::TSOpVector mean_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(mean_out.size(), 1);

    return mean_out;

  }

  ::std::vector<int64_t> dim;
  bool keepdim;
  c10::optional<at::ScalarType> dtype;
  

};

class Min : public TsNode {
 public:
  Min(const torch::lazy::Value& self, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::min),
              {self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(1);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector min_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(min_out.size(), 1);

    return min_out;

  }

  
  

};

class Minimum : public TsNode {
 public:
  Minimum(const torch::lazy::Value& self, const torch::lazy::Value& other, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::minimum),
              {self, other}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(2);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector minimum_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(minimum_out.size(), 1);

    return minimum_out;

  }

  
  

};

class Mm : public TsNode {
 public:
  Mm(const torch::lazy::Value& self, const torch::lazy::Value& mat2, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::mm),
              {self, mat2}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(2);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector mm_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(mm_out.size(), 1);

    return mm_out;

  }

  
  

};

class MulTensor : public TsNode {
 public:
  MulTensor(const torch::lazy::Value& self, const torch::lazy::Value& other, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::mul),
              {self, other}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(2);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector mul_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(mul_out.size(), 1);

    return mul_out;

  }

  
  

};

class Mv : public TsNode {
 public:
  Mv(const torch::lazy::Value& self, const torch::lazy::Value& vec, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::mv),
              {self, vec}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(2);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector mv_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(mv_out.size(), 1);

    return mv_out;

  }

  
  

};

class NativeDropout : public TsNode {
 public:
  NativeDropout(const torch::lazy::Value& input, const double& p, const c10::optional<bool>& train, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::native_dropout),
              {input}, std::move(shapes),
              /* num_outputs */ 2,
              torch::lazy::MHash(p, train)),

        p(p),
        train(train)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", p=" << p;
    if (train.has_value()) {
    ss << ", train=" << train.value();
} else {
    ss << ", train=null";
}
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(3);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back("p", p);
    arguments.emplace_back("train", train);
    
    torch::lazy::TSOpVector native_dropout_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(native_dropout_out.size(), 2);

    return native_dropout_out;

  }

  double p;
  c10::optional<bool> train;
  

};

class NativeDropoutBackward : public TsNode {
 public:
  NativeDropoutBackward(const torch::lazy::Value& grad_output, const torch::lazy::Value& mask, const double& scale, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::native_dropout_backward),
              {grad_output, mask}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(scale)),

        scale(scale)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", scale=" << scale;
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(3);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back("scale", scale);
    
    torch::lazy::TSOpVector native_dropout_backward_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(native_dropout_backward_out.size(), 1);

    return native_dropout_backward_out;

  }

  double scale;
  

};

class NativeLayerNorm : public TsNode {
 public:
  NativeLayerNorm(const torch::lazy::Value& input, const ::std::vector<int64_t>& normalized_shape, const c10::optional<torch::lazy::Value>& weight, const c10::optional<torch::lazy::Value>& bias, const double& eps, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::native_layer_norm),
              {input, weight.value_or(kNullValue), bias.value_or(kNullValue)}, std::move(shapes),
              /* num_outputs */ 3,
              torch::lazy::MHash(normalized_shape, eps)),

        normalized_shape(normalized_shape),
        eps(eps)

  {
    has_weight = !!weight;
    has_bias = !!bias;
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", normalized_shape=" << normalized_shape;
    ss << ", eps=" << eps;
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(5);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back("normalized_shape", normalized_shape);
    arguments.emplace_back(has_weight ? loctx->GetOutputOp(operand(i++)) : nullptr);
    arguments.emplace_back(has_bias ? loctx->GetOutputOp(operand(i++)) : nullptr);
    arguments.emplace_back("eps", eps);
    
    torch::lazy::TSOpVector native_layer_norm_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(native_layer_norm_out.size(), 3);

    return native_layer_norm_out;

  }

  ::std::vector<int64_t> normalized_shape;
  double eps;
  bool has_weight: 1;
  bool has_bias: 1;

};

class NativeLayerNormBackward : public TsNode {
 public:
  NativeLayerNormBackward(const torch::lazy::Value& grad_out, const torch::lazy::Value& input, const ::std::vector<int64_t>& normalized_shape, const torch::lazy::Value& mean, const torch::lazy::Value& rstd, const c10::optional<torch::lazy::Value>& weight, const c10::optional<torch::lazy::Value>& bias, const ::std::vector<bool>& output_mask, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::native_layer_norm_backward),
              {grad_out, input, mean, rstd, weight.value_or(kNullValue), bias.value_or(kNullValue)}, std::move(shapes),
              /* num_outputs */ 3,
              torch::lazy::MHash(normalized_shape, output_mask)),

        normalized_shape(normalized_shape),
        output_mask(output_mask)

  {
    has_weight = !!weight;
    has_bias = !!bias;
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", normalized_shape=" << normalized_shape;
    ss << ", output_mask=" << output_mask;
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(8);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back("normalized_shape", normalized_shape);
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(has_weight ? loctx->GetOutputOp(operand(i++)) : nullptr);
    arguments.emplace_back(has_bias ? loctx->GetOutputOp(operand(i++)) : nullptr);
    arguments.emplace_back("output_mask", output_mask);
    
    torch::lazy::TSOpVector native_layer_norm_backward_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(native_layer_norm_backward_out.size(), 3);

    return native_layer_norm_backward_out;

  }

  ::std::vector<int64_t> normalized_shape;
  ::std::vector<bool> output_mask;
  bool has_weight: 1;
  bool has_bias: 1;

};

class NeScalar : public TsNode {
 public:
  NeScalar(const torch::lazy::Value& self, const torch::lazy::Value& other, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::ne),
              {self, other}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(2);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector ne_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(ne_out.size(), 1);

    return ne_out;

  }

  
  

};

class NeTensor : public TsNode {
 public:
  NeTensor(const torch::lazy::Value& self, const torch::lazy::Value& other, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::ne),
              {self, other}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(2);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector ne_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(ne_out.size(), 1);

    return ne_out;

  }

  
  

};

class Neg : public TsNode {
 public:
  Neg(const torch::lazy::Value& self, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::neg),
              {self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(1);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector neg_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(neg_out.size(), 1);

    return neg_out;

  }

  
  

};

class NllLoss2dBackward : public TsNode {
 public:
  NllLoss2dBackward(const torch::lazy::Value& grad_output, const torch::lazy::Value& self, const torch::lazy::Value& target, const c10::optional<torch::lazy::Value>& weight, const int64_t& reduction, const int64_t& ignore_index, const torch::lazy::Value& total_weight, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::nll_loss2d_backward),
              {grad_output, self, target, weight.value_or(kNullValue), total_weight}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(reduction, ignore_index)),

        reduction(reduction),
        ignore_index(ignore_index)

  {
    has_weight = !!weight;
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", reduction=" << reduction;
    ss << ", ignore_index=" << ignore_index;
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(7);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(has_weight ? loctx->GetOutputOp(operand(i++)) : nullptr);
    arguments.emplace_back("reduction", reduction);
    arguments.emplace_back("ignore_index", ignore_index);
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector nll_loss2d_backward_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(nll_loss2d_backward_out.size(), 1);

    return nll_loss2d_backward_out;

  }

  int64_t reduction;
  int64_t ignore_index;
  bool has_weight: 1;

};

class NllLoss2dForward : public TsNode {
 public:
  NllLoss2dForward(const torch::lazy::Value& self, const torch::lazy::Value& target, const c10::optional<torch::lazy::Value>& weight, const int64_t& reduction, const int64_t& ignore_index, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::nll_loss2d_forward),
              {self, target, weight.value_or(kNullValue)}, std::move(shapes),
              /* num_outputs */ 2,
              torch::lazy::MHash(reduction, ignore_index)),

        reduction(reduction),
        ignore_index(ignore_index)

  {
    has_weight = !!weight;
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", reduction=" << reduction;
    ss << ", ignore_index=" << ignore_index;
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(5);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(has_weight ? loctx->GetOutputOp(operand(i++)) : nullptr);
    arguments.emplace_back("reduction", reduction);
    arguments.emplace_back("ignore_index", ignore_index);
    
    torch::lazy::TSOpVector nll_loss2d_forward_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(nll_loss2d_forward_out.size(), 2);

    return nll_loss2d_forward_out;

  }

  int64_t reduction;
  int64_t ignore_index;
  bool has_weight: 1;

};

class NllLossBackward : public TsNode {
 public:
  NllLossBackward(const torch::lazy::Value& grad_output, const torch::lazy::Value& self, const torch::lazy::Value& target, const c10::optional<torch::lazy::Value>& weight, const int64_t& reduction, const int64_t& ignore_index, const torch::lazy::Value& total_weight, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::nll_loss_backward),
              {grad_output, self, target, weight.value_or(kNullValue), total_weight}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(reduction, ignore_index)),

        reduction(reduction),
        ignore_index(ignore_index)

  {
    has_weight = !!weight;
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", reduction=" << reduction;
    ss << ", ignore_index=" << ignore_index;
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(7);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(has_weight ? loctx->GetOutputOp(operand(i++)) : nullptr);
    arguments.emplace_back("reduction", reduction);
    arguments.emplace_back("ignore_index", ignore_index);
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector nll_loss_backward_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(nll_loss_backward_out.size(), 1);

    return nll_loss_backward_out;

  }

  int64_t reduction;
  int64_t ignore_index;
  bool has_weight: 1;

};

class NllLossForward : public TsNode {
 public:
  NllLossForward(const torch::lazy::Value& self, const torch::lazy::Value& target, const c10::optional<torch::lazy::Value>& weight, const int64_t& reduction, const int64_t& ignore_index, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::nll_loss_forward),
              {self, target, weight.value_or(kNullValue)}, std::move(shapes),
              /* num_outputs */ 2,
              torch::lazy::MHash(reduction, ignore_index)),

        reduction(reduction),
        ignore_index(ignore_index)

  {
    has_weight = !!weight;
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", reduction=" << reduction;
    ss << ", ignore_index=" << ignore_index;
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(5);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(has_weight ? loctx->GetOutputOp(operand(i++)) : nullptr);
    arguments.emplace_back("reduction", reduction);
    arguments.emplace_back("ignore_index", ignore_index);
    
    torch::lazy::TSOpVector nll_loss_forward_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(nll_loss_forward_out.size(), 2);

    return nll_loss_forward_out;

  }

  int64_t reduction;
  int64_t ignore_index;
  bool has_weight: 1;

};

class NormScalaroptDim : public TsNode {
 public:
  NormScalaroptDim(const torch::lazy::Value& self, const c10::optional<torch::lazy::Value>& p, const ::std::vector<int64_t>& dim, const bool& keepdim, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::norm),
              {self, p.value_or(kNullValue)}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(dim, keepdim)),

        dim(dim),
        keepdim(keepdim)

  {
    has_p = !!p;
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", dim=" << dim;
    ss << ", keepdim=" << keepdim;
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(4);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(has_p ? loctx->GetOutputOp(operand(i++)) : nullptr);
    arguments.emplace_back("dim", dim);
    arguments.emplace_back("keepdim", keepdim);
    
    torch::lazy::TSOpVector norm_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(norm_out.size(), 1);

    return norm_out;

  }

  ::std::vector<int64_t> dim;
  bool keepdim;
  bool has_p: 1;

};

class PowTensorTensor : public TsNode {
 public:
  PowTensorTensor(const torch::lazy::Value& self, const torch::lazy::Value& exponent, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::pow),
              {self, exponent}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(2);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector pow_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(pow_out.size(), 1);

    return pow_out;

  }

  
  

};

class PowTensorScalar : public TsNode {
 public:
  PowTensorScalar(const torch::lazy::Value& self, const torch::lazy::Value& exponent, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::pow),
              {self, exponent}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(2);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector pow_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(pow_out.size(), 1);

    return pow_out;

  }

  
  

};

class RandomFrom : public TsNode {
 public:
  RandomFrom(const torch::lazy::Value& self, const int64_t& from, const c10::optional<int64_t>& to, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::random_),
              {self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(from, to)),

        from(from),
        to(to)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", from=" << from;
    if (to.has_value()) {
    ss << ", to=" << to.value();
} else {
    ss << ", to=null";
}
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(3);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back("from", from);
    arguments.emplace_back("to", to);
    
    torch::lazy::TSOpVector random__out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(random__out.size(), 1);

    return random__out;

  }

  int64_t from;
  c10::optional<int64_t> to;
  

};

class RandomTo : public TsNode {
 public:
  RandomTo(const torch::lazy::Value& self, const int64_t& to, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::random_),
              {self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(to)),

        to(to)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", to=" << to;
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(2);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back("to", to);
    
    torch::lazy::TSOpVector random__out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(random__out.size(), 1);

    return random__out;

  }

  int64_t to;
  

};

class Random : public TsNode {
 public:
  Random(const torch::lazy::Value& self, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::random_),
              {self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(1);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector random__out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(random__out.size(), 1);

    return random__out;

  }

  
  

};

class Reciprocal : public TsNode {
 public:
  Reciprocal(const torch::lazy::Value& self, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::reciprocal),
              {self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(1);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector reciprocal_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(reciprocal_out.size(), 1);

    return reciprocal_out;

  }

  
  

};

class Relu : public TsNode {
 public:
  Relu(const torch::lazy::Value& self, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::relu),
              {self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(1);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector relu_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(relu_out.size(), 1);

    return relu_out;

  }

  
  

};

class RemainderTensor : public TsNode {
 public:
  RemainderTensor(const torch::lazy::Value& self, const torch::lazy::Value& other, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::remainder),
              {self, other}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(2);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector remainder_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(remainder_out.size(), 1);

    return remainder_out;

  }

  
  

};

class Repeat : public TsNode {
 public:
  Repeat(const torch::lazy::Value& self, const ::std::vector<int64_t>& repeats, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::repeat),
              {self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(repeats)),

        repeats(repeats)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", repeats=" << repeats;
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(2);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back("repeats", repeats);
    
    torch::lazy::TSOpVector repeat_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(repeat_out.size(), 1);

    return repeat_out;

  }

  ::std::vector<int64_t> repeats;
  

};

class Rsqrt : public TsNode {
 public:
  Rsqrt(const torch::lazy::Value& self, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::rsqrt),
              {self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(1);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector rsqrt_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(rsqrt_out.size(), 1);

    return rsqrt_out;

  }

  
  

};

class ScatterAdd : public TsNode {
 public:
  ScatterAdd(const torch::lazy::Value& self, const int64_t& dim, const torch::lazy::Value& index, const torch::lazy::Value& src, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::scatter_add),
              {self, index, src}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(dim)),

        dim(dim)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", dim=" << dim;
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(4);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back("dim", dim);
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector scatter_add_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(scatter_add_out.size(), 1);

    return scatter_add_out;

  }

  int64_t dim;
  

};

class Sgn : public TsNode {
 public:
  Sgn(const torch::lazy::Value& self, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::sgn),
              {self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(1);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector sgn_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(sgn_out.size(), 1);

    return sgn_out;

  }

  
  

};

class Sigmoid : public TsNode {
 public:
  Sigmoid(const torch::lazy::Value& self, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::sigmoid),
              {self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(1);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector sigmoid_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(sigmoid_out.size(), 1);

    return sigmoid_out;

  }

  
  

};

class SigmoidBackward : public TsNode {
 public:
  SigmoidBackward(const torch::lazy::Value& grad_output, const torch::lazy::Value& output, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(c10::Symbol::fromQualString("aten::sigmoid_backward")),
              {grad_output, output}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(2);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector sigmoid_backward_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(sigmoid_backward_out.size(), 1);

    return sigmoid_backward_out;

  }

  
  

};

class Silu : public TsNode {
 public:
  Silu(const torch::lazy::Value& self, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::silu),
              {self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(1);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector silu_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(silu_out.size(), 1);

    return silu_out;

  }

  
  

};

class SmoothL1Loss : public TsNode {
 public:
  SmoothL1Loss(const torch::lazy::Value& self, const torch::lazy::Value& target, const int64_t& reduction, const double& beta, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::smooth_l1_loss),
              {self, target}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(reduction, beta)),

        reduction(reduction),
        beta(beta)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", reduction=" << reduction;
    ss << ", beta=" << beta;
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(4);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back("reduction", reduction);
    arguments.emplace_back("beta", beta);
    
    torch::lazy::TSOpVector smooth_l1_loss_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(smooth_l1_loss_out.size(), 1);

    return smooth_l1_loss_out;

  }

  int64_t reduction;
  double beta;
  

};

class SmoothL1LossBackward : public TsNode {
 public:
  SmoothL1LossBackward(const torch::lazy::Value& grad_output, const torch::lazy::Value& self, const torch::lazy::Value& target, const int64_t& reduction, const double& beta, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::smooth_l1_loss_backward),
              {grad_output, self, target}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(reduction, beta)),

        reduction(reduction),
        beta(beta)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", reduction=" << reduction;
    ss << ", beta=" << beta;
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(5);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back("reduction", reduction);
    arguments.emplace_back("beta", beta);
    
    torch::lazy::TSOpVector smooth_l1_loss_backward_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(smooth_l1_loss_backward_out.size(), 1);

    return smooth_l1_loss_backward_out;

  }

  int64_t reduction;
  double beta;
  

};

class Softplus : public TsNode {
 public:
  Softplus(const torch::lazy::Value& self, const torch::lazy::Value& beta, const torch::lazy::Value& threshold, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::softplus),
              {self, beta, threshold}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(3);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector softplus_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(softplus_out.size(), 1);

    return softplus_out;

  }

  
  

};

class SoftplusBackward : public TsNode {
 public:
  SoftplusBackward(const torch::lazy::Value& grad_output, const torch::lazy::Value& self, const torch::lazy::Value& beta, const torch::lazy::Value& threshold, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::softplus_backward),
              {grad_output, self, beta, threshold}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(4);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector softplus_backward_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(softplus_backward_out.size(), 1);

    return softplus_backward_out;

  }

  
  

};

class Sort : public TsNode {
 public:
  Sort(const torch::lazy::Value& self, const int64_t& dim, const bool& descending, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::sort),
              {self}, std::move(shapes),
              /* num_outputs */ 2,
              torch::lazy::MHash(dim, descending)),

        dim(dim),
        descending(descending)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", dim=" << dim;
    ss << ", descending=" << descending;
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(3);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back("dim", dim);
    arguments.emplace_back("descending", descending);
    
    torch::lazy::TSOpVector sort_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(sort_out.size(), 2);

    return sort_out;

  }

  int64_t dim;
  bool descending;
  

};

class Sqrt : public TsNode {
 public:
  Sqrt(const torch::lazy::Value& self, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::sqrt),
              {self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(1);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector sqrt_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(sqrt_out.size(), 1);

    return sqrt_out;

  }

  
  

};

class Stack : public TsNode {
 public:
  Stack(const torch::lazy::Value& tensors, const int64_t& dim, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::stack),
              {tensors}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(dim)),

        dim(dim)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", dim=" << dim;
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(2);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back("dim", dim);
    
    torch::lazy::TSOpVector stack_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(stack_out.size(), 1);

    return stack_out;

  }

  int64_t dim;
  

};

class Std : public TsNode {
 public:
  Std(const torch::lazy::Value& self, const bool& unbiased, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::std),
              {self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(unbiased)),

        unbiased(unbiased)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", unbiased=" << unbiased;
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(2);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back("unbiased", unbiased);
    
    torch::lazy::TSOpVector std_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(std_out.size(), 1);

    return std_out;

  }

  bool unbiased;
  

};

class StdDim : public TsNode {
 public:
  StdDim(const torch::lazy::Value& self, const ::std::vector<int64_t>& dim, const bool& unbiased, const bool& keepdim, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::std),
              {self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(dim, unbiased, keepdim)),

        dim(dim),
        unbiased(unbiased),
        keepdim(keepdim)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", dim=" << dim;
    ss << ", unbiased=" << unbiased;
    ss << ", keepdim=" << keepdim;
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(4);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back("dim", dim);
    arguments.emplace_back("unbiased", unbiased);
    arguments.emplace_back("keepdim", keepdim);
    
    torch::lazy::TSOpVector std_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(std_out.size(), 1);

    return std_out;

  }

  ::std::vector<int64_t> dim;
  bool unbiased;
  bool keepdim;
  

};

class StdCorrection : public TsNode {
 public:
  StdCorrection(const torch::lazy::Value& self, const c10::optional<::std::vector<int64_t>>& dim, const c10::optional<int64_t>& correction, const bool& keepdim, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::std),
              {self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(dim, correction, keepdim)),

        dim(dim),
        correction(correction),
        keepdim(keepdim)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    if (dim.has_value()) {
    ss << ", dim=" << dim.value();
} else {
    ss << ", dim=null";
}
    if (correction.has_value()) {
    ss << ", correction=" << correction.value();
} else {
    ss << ", correction=null";
}
    ss << ", keepdim=" << keepdim;
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(2);
    kwarguments.reserve(2);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back("dim", dim);
    kwarguments.emplace_back("correction", correction);
    kwarguments.emplace_back("keepdim", keepdim);
    torch::lazy::TSOpVector std_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(std_out.size(), 1);

    return std_out;

  }

  c10::optional<::std::vector<int64_t>> dim;
  c10::optional<int64_t> correction;
  bool keepdim;
  

};

class SubTensor : public TsNode {
 public:
  SubTensor(const torch::lazy::Value& self, const torch::lazy::Value& other, const torch::lazy::Value& alpha, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::sub),
              {self, other, alpha}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(2);
    kwarguments.reserve(1);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    kwarguments.emplace_back("alpha", loctx->GetOutputOp(operand(i++)));
    torch::lazy::TSOpVector sub_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(sub_out.size(), 1);

    return sub_out;

  }

  
  

};

class Sum : public TsNode {
 public:
  Sum(const torch::lazy::Value& self, const c10::optional<at::ScalarType>& dtype, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::sum),
              {self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(dtype)),

        dtype(dtype)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    if (dtype.has_value()) {
    ss << ", dtype=" << dtype.value();
} else {
    ss << ", dtype=null";
}
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(1);
    kwarguments.reserve(1);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    kwarguments.emplace_back("dtype", dtype);
    torch::lazy::TSOpVector sum_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(sum_out.size(), 1);

    return sum_out;

  }

  c10::optional<at::ScalarType> dtype;
  

};

class SumDimIntlist : public TsNode {
 public:
  SumDimIntlist(const torch::lazy::Value& self, const ::std::vector<int64_t>& dim, const bool& keepdim, const c10::optional<at::ScalarType>& dtype, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::sum),
              {self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(dim, keepdim, dtype)),

        dim(dim),
        keepdim(keepdim),
        dtype(dtype)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", dim=" << dim;
    ss << ", keepdim=" << keepdim;
    if (dtype.has_value()) {
    ss << ", dtype=" << dtype.value();
} else {
    ss << ", dtype=null";
}
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(3);
    kwarguments.reserve(1);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back("dim", dim);
    arguments.emplace_back("keepdim", keepdim);
    kwarguments.emplace_back("dtype", dtype);
    torch::lazy::TSOpVector sum_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(sum_out.size(), 1);

    return sum_out;

  }

  ::std::vector<int64_t> dim;
  bool keepdim;
  c10::optional<at::ScalarType> dtype;
  

};

class Tanh : public TsNode {
 public:
  Tanh(const torch::lazy::Value& self, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::tanh),
              {self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(1);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector tanh_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(tanh_out.size(), 1);

    return tanh_out;

  }

  
  

};

class TanhBackward : public TsNode {
 public:
  TanhBackward(const torch::lazy::Value& grad_output, const torch::lazy::Value& output, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::tanh_backward),
              {grad_output, output}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(2);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector tanh_backward_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(tanh_backward_out.size(), 1);

    return tanh_backward_out;

  }

  
  

};

class Threshold : public TsNode {
 public:
  Threshold(const torch::lazy::Value& self, const torch::lazy::Value& threshold, const torch::lazy::Value& value, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::threshold),
              {self, threshold, value}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(3);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector threshold_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(threshold_out.size(), 1);

    return threshold_out;

  }

  
  

};

class ThresholdBackward : public TsNode {
 public:
  ThresholdBackward(const torch::lazy::Value& grad_output, const torch::lazy::Value& self, const torch::lazy::Value& threshold, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::threshold_backward),
              {grad_output, self, threshold}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(3);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector threshold_backward_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(threshold_backward_out.size(), 1);

    return threshold_backward_out;

  }

  
  

};

class Topk : public TsNode {
 public:
  Topk(const torch::lazy::Value& self, const int64_t& k, const int64_t& dim, const bool& largest, const bool& sorted, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::topk),
              {self}, std::move(shapes),
              /* num_outputs */ 2,
              torch::lazy::MHash(k, dim, largest, sorted)),

        k(k),
        dim(dim),
        largest(largest),
        sorted(sorted)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", k=" << k;
    ss << ", dim=" << dim;
    ss << ", largest=" << largest;
    ss << ", sorted=" << sorted;
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(5);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back("k", k);
    arguments.emplace_back("dim", dim);
    arguments.emplace_back("largest", largest);
    arguments.emplace_back("sorted", sorted);
    
    torch::lazy::TSOpVector topk_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(topk_out.size(), 2);

    return topk_out;

  }

  int64_t k;
  int64_t dim;
  bool largest;
  bool sorted;
  

};

class Trace : public TsNode {
 public:
  Trace(const torch::lazy::Value& self, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::trace),
              {self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(1);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector trace_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(trace_out.size(), 1);

    return trace_out;

  }

  
  

};

class Tril : public TsNode {
 public:
  Tril(const torch::lazy::Value& self, const int64_t& diagonal, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::tril),
              {self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(diagonal)),

        diagonal(diagonal)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", diagonal=" << diagonal;
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(2);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back("diagonal", diagonal);
    
    torch::lazy::TSOpVector tril_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(tril_out.size(), 1);

    return tril_out;

  }

  int64_t diagonal;
  

};

class Triu : public TsNode {
 public:
  Triu(const torch::lazy::Value& self, const int64_t& diagonal, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::triu),
              {self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(diagonal)),

        diagonal(diagonal)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", diagonal=" << diagonal;
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(2);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back("diagonal", diagonal);
    
    torch::lazy::TSOpVector triu_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(triu_out.size(), 1);

    return triu_out;

  }

  int64_t diagonal;
  

};

class Trunc : public TsNode {
 public:
  Trunc(const torch::lazy::Value& self, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::trunc),
              {self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(1);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector trunc_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(trunc_out.size(), 1);

    return trunc_out;

  }

  
  

};

class UpsampleBilinear2d : public TsNode {
 public:
  UpsampleBilinear2d(const torch::lazy::Value& self, const ::std::vector<int64_t>& output_size, const bool& align_corners, const c10::optional<double>& scales_h, const c10::optional<double>& scales_w, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::upsample_bilinear2d),
              {self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(output_size, align_corners, scales_h, scales_w)),

        output_size(output_size),
        align_corners(align_corners),
        scales_h(scales_h),
        scales_w(scales_w)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", output_size=" << output_size;
    ss << ", align_corners=" << align_corners;
    if (scales_h.has_value()) {
    ss << ", scales_h=" << scales_h.value();
} else {
    ss << ", scales_h=null";
}
    if (scales_w.has_value()) {
    ss << ", scales_w=" << scales_w.value();
} else {
    ss << ", scales_w=null";
}
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(5);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back("output_size", output_size);
    arguments.emplace_back("align_corners", align_corners);
    arguments.emplace_back("scales_h", scales_h);
    arguments.emplace_back("scales_w", scales_w);
    
    torch::lazy::TSOpVector upsample_bilinear2d_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(upsample_bilinear2d_out.size(), 1);

    return upsample_bilinear2d_out;

  }

  ::std::vector<int64_t> output_size;
  bool align_corners;
  c10::optional<double> scales_h;
  c10::optional<double> scales_w;
  

};

class UpsampleBilinear2dBackward : public TsNode {
 public:
  UpsampleBilinear2dBackward(const torch::lazy::Value& grad_output, const ::std::vector<int64_t>& output_size, const ::std::vector<int64_t>& input_size, const bool& align_corners, const c10::optional<double>& scales_h, const c10::optional<double>& scales_w, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::upsample_bilinear2d_backward),
              {grad_output}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(output_size, input_size, align_corners, scales_h, scales_w)),

        output_size(output_size),
        input_size(input_size),
        align_corners(align_corners),
        scales_h(scales_h),
        scales_w(scales_w)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", output_size=" << output_size;
    ss << ", input_size=" << input_size;
    ss << ", align_corners=" << align_corners;
    if (scales_h.has_value()) {
    ss << ", scales_h=" << scales_h.value();
} else {
    ss << ", scales_h=null";
}
    if (scales_w.has_value()) {
    ss << ", scales_w=" << scales_w.value();
} else {
    ss << ", scales_w=null";
}
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(6);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back("output_size", output_size);
    arguments.emplace_back("input_size", input_size);
    arguments.emplace_back("align_corners", align_corners);
    arguments.emplace_back("scales_h", scales_h);
    arguments.emplace_back("scales_w", scales_w);
    
    torch::lazy::TSOpVector upsample_bilinear2d_backward_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(upsample_bilinear2d_backward_out.size(), 1);

    return upsample_bilinear2d_backward_out;

  }

  ::std::vector<int64_t> output_size;
  ::std::vector<int64_t> input_size;
  bool align_corners;
  c10::optional<double> scales_h;
  c10::optional<double> scales_w;
  

};

class UpsampleNearest2d : public TsNode {
 public:
  UpsampleNearest2d(const torch::lazy::Value& self, const ::std::vector<int64_t>& output_size, const c10::optional<double>& scales_h, const c10::optional<double>& scales_w, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::upsample_nearest2d),
              {self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(output_size, scales_h, scales_w)),

        output_size(output_size),
        scales_h(scales_h),
        scales_w(scales_w)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", output_size=" << output_size;
    if (scales_h.has_value()) {
    ss << ", scales_h=" << scales_h.value();
} else {
    ss << ", scales_h=null";
}
    if (scales_w.has_value()) {
    ss << ", scales_w=" << scales_w.value();
} else {
    ss << ", scales_w=null";
}
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(4);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back("output_size", output_size);
    arguments.emplace_back("scales_h", scales_h);
    arguments.emplace_back("scales_w", scales_w);
    
    torch::lazy::TSOpVector upsample_nearest2d_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(upsample_nearest2d_out.size(), 1);

    return upsample_nearest2d_out;

  }

  ::std::vector<int64_t> output_size;
  c10::optional<double> scales_h;
  c10::optional<double> scales_w;
  

};

class UpsampleNearest2dBackward : public TsNode {
 public:
  UpsampleNearest2dBackward(const torch::lazy::Value& grad_output, const ::std::vector<int64_t>& output_size, const ::std::vector<int64_t>& input_size, const c10::optional<double>& scales_h, const c10::optional<double>& scales_w, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::upsample_nearest2d_backward),
              {grad_output}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash(output_size, input_size, scales_h, scales_w)),

        output_size(output_size),
        input_size(input_size),
        scales_h(scales_h),
        scales_w(scales_w)

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    ss << ", output_size=" << output_size;
    ss << ", input_size=" << input_size;
    if (scales_h.has_value()) {
    ss << ", scales_h=" << scales_h.value();
} else {
    ss << ", scales_h=null";
}
    if (scales_w.has_value()) {
    ss << ", scales_w=" << scales_w.value();
} else {
    ss << ", scales_w=null";
}
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(5);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    arguments.emplace_back("output_size", output_size);
    arguments.emplace_back("input_size", input_size);
    arguments.emplace_back("scales_h", scales_h);
    arguments.emplace_back("scales_w", scales_w);
    
    torch::lazy::TSOpVector upsample_nearest2d_backward_out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(upsample_nearest2d_backward_out.size(), 1);

    return upsample_nearest2d_backward_out;

  }

  ::std::vector<int64_t> output_size;
  ::std::vector<int64_t> input_size;
  c10::optional<double> scales_h;
  c10::optional<double> scales_w;
  

};

class Zero : public TsNode {
 public:
  Zero(const torch::lazy::Value& self, std::vector<Shape>&& shapes)
      : TsNode(torch::lazy::OpKind(at::aten::zero_),
              {self}, std::move(shapes),
              /* num_outputs */ 1,
              torch::lazy::MHash())
        

  {
    
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << TsNode::ToString();
    
    return ss.str();
  }

  torch::lazy::TSOpVector Lower(std::shared_ptr<torch::jit::GraphFunction> function,
                   torch::lazy::TSLoweringContext* loctx) const override {
        std::vector<torch::jit::NamedValue> arguments;
    std::vector<torch::jit::NamedValue> kwarguments;
    arguments.reserve(1);
    kwarguments.reserve(0);
    size_t i = 0;
    arguments.emplace_back(loctx->GetOutputOp(operand(i++)));
    
    torch::lazy::TSOpVector zero__out = torch::lazy::LowerTSBuiltin(function, op().op, arguments, kwarguments);
    CHECK_EQ(zero__out.size(), 1);

    return zero__out;

  }

  
  

};

} // namespace lazy
} // namespace torch
